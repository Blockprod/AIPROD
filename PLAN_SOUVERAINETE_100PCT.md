# PLAN SOUVERAINET√â 100% PROPRI√âTAIRE ‚Äî AIPROD
## De ¬´ D√©pendant masqu√© ¬ª √† ¬´ 100% Propri√©taire r√©el ¬ª

**Date :** 2026-02-15  
**Objectif :** Fusionner le meilleur de AIPROD_V33 (orchestration SaaS) et LTX-2 (diffusion vid√©o) dans un syst√®me **enti√®rement propri√©taire**, entra√Æn√© sur ses propres mod√®les, op√©rable en air-gapped.  
**Horizon :** 12 semaines (3 mois)  
**R√©f√©rence :** `AUDIT_AIPROD_FUSION_100PCT_PROPRIETAIRE.md`

---

## SYNTH√àSE EX√âCUTIVE

```
√âtat actuel  ‚Üí  3/10 souverainet√©  ‚Üí  Pipeline API mock, mod√®les t√©l√©charg√©s dynamiquement
Objectif     ‚Üí  9/10 souverainet√©  ‚Üí  Pipeline r√©el, mod√®les internes, z√©ro API externe
```

### 4 Phases ‚Äî 12 semaines

| Phase | Nom | Dur√©e | Objectif |
|---|---|---|---|
| **Phase 1** | Couper les fils | Semaines 1-2 | √âliminer toutes les d√©pendances externes |
| **Phase 2** | Connecter le moteur | Semaines 3-5 | Brancher le vrai GPU pipeline sur l'API |
| **Phase 3** | Entra√Æner ses mod√®les | Semaines 6-10 | Fine-tuner et poss√©der chaque mod√®le |
| **Phase 4** | Verrouiller et certifier | Semaines 11-12 | Reproductibilit√©, tests, documentation souveraine |

---

## PHASE 1 ‚Äî COUPER LES FILS (Semaines 1-2)

**Objectif :** Z√©ro appel r√©seau sortant vers un service tiers. Tout fonctionne offline.

---

### 1.1 Pr√©-provisionner TOUS les mod√®les localement

**Probl√®me :** 6 appels `from_pretrained()` t√©l√©chargent ~40 GB depuis HuggingFace Hub.  
**Solution :** Script de provisionnement unique + flag `local_files_only=True` partout.

#### Action 1.1.1 ‚Äî Cr√©er `scripts/provision_models.py`

```python
"""
T√©l√©charge et stocke TOUS les mod√®les n√©cessaires en local.
√Ä ex√©cuter UNE FOIS sur machine connect√©e, puis le projet est 100% offline.
"""
from huggingface_hub import snapshot_download
from pathlib import Path
import hashlib, json

MODELS = {
    "models/text-encoder/gemma-3-1b": {
        "repo": "google/gemma-3-1b-pt",
        "revision": "main",  # Figer au commit SHA exact apr√®s t√©l√©chargement
    },
    "models/scenarist/mistral-7b": {
        "repo": "mistralai/Mistral-7B-Instruct-v0.3",
        "revision": "main",
    },
    "models/captioning/qwen-omni-7b": {
        "repo": "Qwen/Qwen2.5-Omni-7B",
        "revision": "main",
    },
}

def download_all():
    manifest = {}
    for local_path, spec in MODELS.items():
        path = Path(local_path)
        path.mkdir(parents=True, exist_ok=True)
        print(f"Downloading {spec['repo']} ‚Üí {local_path}")
        snapshot_download(
            repo_id=spec["repo"],
            local_dir=str(path),
            revision=spec["revision"],
        )
        # G√©n√©rer hash de v√©rification
        manifest[local_path] = {
            "repo": spec["repo"],
            "revision": spec["revision"],
            "files": [str(f) for f in path.rglob("*.safetensors")],
        }
    
    # Sauvegarder le manifeste
    with open("models/MANIFEST.json", "w") as f:
        json.dump(manifest, f, indent=2)
    print("‚úÖ Tous les mod√®les provisionn√©s. MANIFEST.json cr√©√©.")

if __name__ == "__main__":
    download_all()
```

#### Action 1.1.2 ‚Äî Forcer `local_files_only=True` partout

| Fichier | Modification |
|---|---|
| `packages/aiprod-core/src/aiprod_core/model/text_encoder/bridge.py` | `AutoModel.from_pretrained(..., local_files_only=True)` + `AutoTokenizer.from_pretrained(..., local_files_only=True)` |
| `packages/aiprod-pipelines/src/aiprod_pipelines/inference/scenarist/scenarist.py` | `AutoModelForCausalLM.from_pretrained(..., local_files_only=True)` + `AutoTokenizer.from_pretrained(..., local_files_only=True)` |
| `packages/aiprod-trainer/src/aiprod_trainer/captioning.py` | `from_pretrained(..., local_files_only=True)` dans `QwenOmniCaptioner` |
| `packages/aiprod-trainer/src/aiprod_trainer/gemma_8bit.py` | ‚úÖ D√©j√† fait |

#### Action 1.1.3 ‚Äî Mettre √† jour les chemins par d√©faut

| Fichier | Ancien default | Nouveau default |
|---|---|---|
| `bridge.py` ‚Üí `LLMBridgeConfig.model_name` | `"meta-llama/Llama-3.2-1B"` | `"models/text-encoder/gemma-3-1b"` |
| `scenarist.py` ‚Üí `LLMScenarist` | `"mistralai/Mistral-7B-Instruct-v0.3"` | `"models/scenarist/mistral-7b"` |
| `captioning.py` ‚Üí `QwenOmniCaptioner.MODEL_ID` | `"Qwen/Qwen2.5-Omni-7B"` | `"models/captioning/qwen-omni-7b"` |

**Crit√®re de validation :** `python -c "from aiprod_core.model.text_encoder import LLMBridge; b = LLMBridge(); b.encode_text('test')"` fonctionne **sans connexion r√©seau**.

---

### 1.2 Supprimer la d√©pendance Google Gemini

**Probl√®me :** `google.generativeai` import√© en dur dans 2 fichiers. Appels API vers Google.

#### Action 1.2.1 ‚Äî Rendre `gemini_client.py` optionnel

```
Fichier : packages/aiprod-pipelines/src/aiprod_pipelines/api/integrations/gemini_client.py
```

- Transformer `import google.generativeai as genai` en `try/except ImportError`
- Le mode mock (d√©j√† impl√©ment√©) devient le **mode par d√©faut**
- Si le SDK Google est install√© ET une cl√© API fournie ‚Üí mode live (opt-in explicite)

#### Action 1.2.2 ‚Äî Remplacer `GeminiFlashCaptioner` comme captioner par d√©faut

```
Fichier : packages/aiprod-trainer/src/aiprod_trainer/captioning.py
```

- `GeminiFlashCaptioner` ‚Üí d√©plac√© dans un module optionnel `captioning_external.py`
- Le captioner par d√©faut devient `QwenOmniCaptioner` (local) ou `CachedCaptioner`
- Google Gemini = import optionnel, jamais charg√© par d√©faut

#### Action 1.2.3 ‚Äî Nettoyer le Dockerfile principal

```
Fichier : deploy/docker/Dockerfile
```

- Retirer `google-generativeai==0.3.0` de la ligne pip install
- Retirer `google-cloud-logging==3.8.0` et `google-cloud-monitoring==2.16.0`
- Garder uniquement les d√©pendances souveraines

**Crit√®re de validation :** `pip install` du projet fonctionne sans aucun package `google-*`.

---

### 1.3 Isoler les backends cloud en modules optionnels

**Probl√®me :** `boto3`, `google-cloud-storage`, `stripe` sont dans les requirements globaux.

#### Action 1.3.1 ‚Äî Cr√©er des extras optionnels dans `pyproject.toml`

```toml
[project.optional-dependencies]
cloud-gcs = ["google-cloud-storage>=2.10"]
cloud-s3 = ["boto3>=1.35"]
billing-stripe = ["stripe>=7.0"]
tracking-wandb = ["wandb>=0.16"]
tracking-gemini = ["google-generativeai>=0.3"]
```

#### Action 1.3.2 ‚Äî Guard imports dans le code

| Fichier | Import actuel | Import s√©curis√© |
|---|---|---|
| `streaming/sources.py` ‚Üí `S3DataSource` | `import boto3` en dur | `try: import boto3 except ImportError: raise ...` |
| `streaming/sources.py` ‚Üí `GCSDataSource` | `from google.cloud import storage` en dur | `try/except ImportError` |
| `billing_service.py` ‚Üí `StripeIntegration` | ‚úÖ D√©j√† en try/except | OK |
| `vae_trainer.py` ‚Üí `wandb` | `import wandb` en dur | `try/except ImportError` avec fallback console |

#### Action 1.3.3 ‚Äî Mettre √† jour `requirements.txt`

```
# CORE (100% souverain ‚Äî aucune d√©pendance SaaS)
torch==2.5.1
torchvision==0.20.1
torchaudio==2.5.1
transformers==4.47.0
safetensors==0.4.5
accelerate==1.2.0
peft==0.14.0
einops==0.8.0
numpy==1.26.4
scipy==1.14.0
pillow==10.4.0
opencv-python==4.10.0.84
imageio==2.36.0
av==13.1.0
librosa==0.10.2
matplotlib==3.9.3
pydantic==2.10.0
fastapi==0.115.0
uvicorn==0.32.0
aiohttp==3.11.0
rich==13.9.0
typer==0.14.0
tqdm==4.67.0
pyyaml==6.0.2
cachetools==5.5.0
scenedetect==0.6.4
lpips==0.1.4
zstandard==0.23.0
xformers==0.0.28
bitsandbytes==0.45.0
optimum-quanto==0.2.6

# OBSERVABILIT√â (auto-h√©bergeable)
prometheus-client==0.21.0
opentelemetry-api==1.29.0
opentelemetry-sdk==1.29.0
opentelemetry-exporter-otlp==1.29.0
structlog==24.4.0
mlflow==2.19.0

# OPTIONNEL (installer s√©par√©ment si besoin)
# pip install aiprod[cloud-gcs]    ‚Üí google-cloud-storage
# pip install aiprod[cloud-s3]     ‚Üí boto3
# pip install aiprod[billing]      ‚Üí stripe
# pip install aiprod[tracking]     ‚Üí wandb
# pip install aiprod[gemini]       ‚Üí google-generativeai
```

**Crit√®re de validation :** `pip install -r requirements.txt` n'installe AUCUN package Google, AWS, Stripe ou Wandb.

---

### 1.4 Figer les versions et cr√©er un lockfile

#### Action 1.4.1

```bash
# G√©n√©rer le lockfile complet
pip freeze > requirements.lock

# V√©rifier l'int√©grit√©
pip install --no-deps -r requirements.lock  # doit √™tre identique
```

#### Action 1.4.2 ‚Äî Hasher les poids de mod√®les

Cr√©er `scripts/verify_model_integrity.py` :
- SHA-256 de chaque fichier `.safetensors` et `.pt`
- Stocker dans `models/CHECKSUMS.sha256`
- V√©rification automatique au d√©marrage du pipeline

**Crit√®re de validation :** `python scripts/verify_model_integrity.py` retourne ‚úÖ pour chaque fichier.

---

## PHASE 2 ‚Äî CONNECTER LE MOTEUR (Semaines 3-5)

**Objectif :** Le pipeline API g√©n√®re de VRAIES vid√©os sur GPU local. Z√©ro mock.

---

### 2.1 Impl√©menter le GPU Worker

**Probl√®me :** Le gateway enqueue des jobs mais rien ne les traite. Le vrai moteur (`ti2vid_one_stage.py`) est un CLI d√©connect√©.

#### Action 2.1.1 ‚Äî Cr√©er `packages/aiprod-pipelines/src/aiprod_pipelines/api/gpu_worker.py`

**Architecture :**

```
Gateway (FastAPI, CPU)
    ‚îÇ
    ‚ñº (Redis queue ou in-memory queue)
    ‚îÇ
GPU Worker (consomme les jobs)
    ‚îÇ
    ‚îú‚îÄ‚îÄ Charge TI2VidOneStagePipeline (une fois au boot)
    ‚îÇ   ‚îú‚îÄ‚îÄ Transformer SHDT (25 GB, FP8)
    ‚îÇ   ‚îú‚îÄ‚îÄ Video VAE Encoder/Decoder
    ‚îÇ   ‚îú‚îÄ‚îÄ Audio VAE + Vocoder
    ‚îÇ   ‚îî‚îÄ‚îÄ Text Encoder (Gemma local)
    ‚îÇ
    ‚îú‚îÄ‚îÄ Pour chaque job :
    ‚îÇ   ‚îú‚îÄ‚îÄ Valide le prompt (InputSanitizer)
    ‚îÇ   ‚îú‚îÄ‚îÄ D√©compose en scenes (Scenarist ‚Äî local LLM ou rule-based)
    ‚îÇ   ‚îú‚îÄ‚îÄ G√©n√®re chaque clip (pipeline.generate())
    ‚îÇ   ‚îú‚îÄ‚îÄ Assemble les clips (ffmpeg)
    ‚îÇ   ‚îú‚îÄ‚îÄ QA technique (resolution, codec, dur√©e)
    ‚îÇ   ‚îî‚îÄ‚îÄ Retourne le r√©sultat (fichier vid√©o + m√©tadonn√©es)
    ‚îÇ
    ‚îî‚îÄ‚îÄ Sauvegarde le r√©sultat
        ‚îú‚îÄ‚îÄ Syst√®me de fichiers local
        ‚îî‚îÄ‚îÄ Webhook notification au client
```

**Sp√©cification du worker :**

```python
class GPUWorker:
    """
    Worker GPU souverain ‚Äî consomme les jobs de la queue et g√©n√®re de vraies vid√©os.
    """
    
    def __init__(self, config: WorkerConfig):
        # Charger le pipeline UNE FOIS au d√©marrage
        self.pipeline = TI2VidOneStagePipeline(
            checkpoint_path=config.checkpoint_path,        # models/ltx2_research/
            gemma_root=config.text_encoder_path,           # models/text-encoder/gemma-3-1b/
            loras=[],
            fp8transformer=True,
        )
        self.scenarist = RuleBasedDecomposer()             # 100% local, pas de LLM
        self.output_dir = Path(config.output_dir)
    
    async def process_job(self, job: Job) -> JobResult:
        """Traite un job de g√©n√©ration vid√©o."""
        # 1. D√©composer le prompt en shots
        storyboard = self.scenarist.decompose(job.prompt)
        
        # 2. G√©n√©rer chaque clip
        clips = []
        for shot in storyboard.shots:
            video, audio = self.pipeline(
                prompt=shot.prompt,
                negative_prompt=job.negative_prompt or "",
                seed=job.seed + shot.index,
                height=job.height or 720,
                width=job.width or 1280,
                num_frames=shot.num_frames,
                frame_rate=job.frame_rate or 24.0,
                num_inference_steps=job.steps or 30,
                video_guider_params=MultiModalGuiderParams(cfg_scale=3.0),
                audio_guider_params=MultiModalGuiderParams(cfg_scale=3.0),
                images=[],
            )
            clips.append((video, audio))
        
        # 3. Encoder et sauvegarder
        output_path = self.output_dir / f"{job.job_id}.mp4"
        encode_video(video=clips[0][0], audio=clips[0][1], ...)
        
        # 4. QA technique
        qa_result = self.technical_qa(output_path)
        
        return JobResult(
            job_id=job.job_id,
            status="completed",
            output_path=str(output_path),
            qa_score=qa_result.score,
        )
```

#### Action 2.1.2 ‚Äî Remplacer le RenderExecutorAdapter mock

```
Fichier : packages/aiprod-pipelines/src/aiprod_pipelines/api/adapters/render.py
```

R√©√©crire `_render_with_backend()` pour appeler le vrai `GPUWorker` au lieu de retourner des assets fabriqu√©s.

**Supprimer :**
- Tous les `random.random()` qui simulent des √©checs
- Les URLs `gs://aiprod-assets/` hardcod√©es
- Le commentaire `"In production, would call actual backend API"`

**Remplacer par :**
- Appel r√©el √† `self.gpu_worker.process_job(job)`
- Le backend unique = `"aiprod_sovereign"` (pas runway, pas replicate, pas veo3)

#### Action 2.1.3 ‚Äî Impl√©menter la queue de jobs persistante

```
Nouveau fichier : packages/aiprod-pipelines/src/aiprod_pipelines/api/job_store.py
```

**Option A (minimaliste) :** SQLite local  
**Option B (production) :** Redis (d√©j√† pr√©vu dans K8s config)

```python
class JobStore:
    """Stockage persistant des jobs ‚Äî survit aux red√©marrages."""
    
    def enqueue(self, job: Job) -> str: ...
    def dequeue(self) -> Optional[Job]: ...
    def update_status(self, job_id: str, status: str, result: dict): ...
    def get_job(self, job_id: str) -> Optional[Job]: ...
    def list_pending(self) -> list[Job]: ...
```

**Crit√®re de validation :**
```bash
# Soumettre un job via l'API
curl -X POST http://localhost:8080/v1/generate \
  -H "Authorization: Bearer $TOKEN" \
  -d '{"prompt": "A cat walking in a garden", "seed": 42}'

# Le r√©sultat est un VRAI fichier vid√©o .mp4, pas un mock
ffprobe output/job_xxx.mp4  # ‚Üí codec h264, dur√©e > 0, r√©solution r√©elle
```

---

### 2.2 R√©√©crire la config architecturale

**Probl√®me :** `AIPROD_V33.json` d√©crit un syst√®me Google-centric qui ne refl√®te plus la r√©alit√©.

#### Action 2.2.1 ‚Äî Cr√©er `config/AIPROD_V34_SOVEREIGN.json`

Changements cl√©s par rapport √† V33 :

| Bloc V33 | Bloc V34 Souverain | Changement |
|---|---|---|
| `creativeDirector` ‚Üí `llmProvider: "google"`, `llmModel: "gemini-1.5-pro"` | `llmProvider: "local"`, `llmModel: "models/scenarist/mistral-7b"` | LLM local |
| `visualTranslator` ‚Üí `llmProvider: "google"` | `llmProvider: "local"`, `llmModel: "models/scenarist/mistral-7b"` | LLM local |
| `inputSanitizer` ‚Üí `googleOptimized: true` | `googleOptimized: false`, `geminiPromptFormat: null` | Suppression Google |
| `renderExecutor` ‚Üí `veo3Configuration`, `runwayGen3`, `replicate_wan25` | `backend: "aiprod_sovereign"`, GPU local via `TI2VidOneStagePipeline` | Rendu local |
| `financialOrchestrator` ‚Üí `backendPriority: ["veo3", "runwayGen3", ...]` | `backendPriority: ["aiprod_sovereign"]` | Backend unique |
| `semanticQA` ‚Üí `llmProvider: "google"`, `visionModel: "gemini-1.5-pro-vision"` | `llmProvider: "local"`, `visionModel: "models/vision-qa/clip-local"` | QA local |
| `supervisor` ‚Üí `llmProvider: "google"` | Score-based rules (pas de LLM) | Suppression LLM pour approbation |
| `googleCloudServices` | **SUPPRIM√â** | Aucun service cloud |
| `googleStackConfiguration` | **SUPPRIM√â** | Aucune cl√© API |
| `dynamicPricing.sources` ‚Üí `["google_cloud_billing", ...]` | `sources: ["internal_gpu_meter"]` | Co√ªts internes |

#### Action 2.2.2 ‚Äî Mettre √† jour l'orchestrateur

```
Fichier : packages/aiprod-pipelines/src/aiprod_pipelines/api/orchestrator.py
```

Charger `AIPROD_V34_SOVEREIGN.json` par d√©faut. Ajouter une variable d'environnement :

```python
CONFIG_PATH = os.environ.get("AIPROD_CONFIG", "config/AIPROD_V34_SOVEREIGN.json")
```

---

### 2.3 Impl√©menter le QA s√©mantique souverain

**Probl√®me :** Le QA s√©mantique dans V33 repose sur `gemini-1.5-pro-vision` (API Google).

#### Action 2.3.1 ‚Äî Cr√©er un QA s√©mantique bas√© sur CLIP local

```
Nouveau fichier : packages/aiprod-pipelines/src/aiprod_pipelines/api/adapters/qa_semantic_local.py
```

**Architecture :**

```python
class LocalSemanticQA:
    """
    QA s√©mantique 100% local ‚Äî √©value la coh√©rence prompt ‚Üî vid√©o g√©n√©r√©e.
    Utilise CLIP (ou SigLIP) en local pour le score de similarit√©.
    """
    
    def __init__(self, model_path: str = "models/qa/siglip-base"):
        self.model = load_clip_model(model_path)  # local_files_only=True
    
    def evaluate(self, video_path: str, prompt: str) -> SemanticQAResult:
        # 1. Extraire des frames cl√©s de la vid√©o
        frames = extract_keyframes(video_path, n=8)
        
        # 2. Calculer la similarit√© CLIP prompt ‚Üî frames
        text_embedding = self.model.encode_text(prompt)
        frame_embeddings = [self.model.encode_image(f) for f in frames]
        similarity_scores = [cosine_sim(text_embedding, fe) for fe in frame_embeddings]
        
        # 3. √âvaluer la coh√©rence temporelle (inter-frames)
        temporal_coherence = compute_temporal_coherence(frame_embeddings)
        
        return SemanticQAResult(
            prompt_adherence=mean(similarity_scores),
            temporal_coherence=temporal_coherence,
            overall_score=weighted_mean(...),
        )
```

**Mod√®le √† pr√©provisionner :** `google/siglip-base-patch16-224` (350 MB, Apache 2.0, local inference)

---

### 2.4 Pipeline complet end-to-end

Une fois Phase 2 termin√©e, le flux est :

```
Client ‚Üí API Gateway ‚Üí Job Queue (SQLite/Redis)
                              ‚îÇ
                              ‚ñº
                        GPU Worker
                              ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚ñº         ‚ñº         ‚ñº
             Scenarist   Text Encode  Noise Init
             (local      (Gemma 3,   (Gaussian,
              Mistral     local)      seeded)
              ou rules)
                    ‚îÇ         ‚îÇ         ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚ñº
                     Diffusion Loop
                     (SHDT Transformer, 
                      Euler steps, CFG,
                      FP8, local GPU)
                              ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚ñº         ‚ñº         ‚ñº
              VAE Decode  Audio Decode  Upsampler
              (HW-VAE)   (NAC Codec)   (√† impl√©menter)
                    ‚îÇ         ‚îÇ         ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚ñº
                      Video Encoding
                      (ffmpeg, H.264)
                              ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚ñº         ‚ñº         ‚ñº
              QA Technique  QA S√©mantique  Billing
              (deterministic) (CLIP local)  (interne)
                              ‚îÇ
                              ‚ñº
                      R√©sultat ‚Üí Client
                      (fichier .mp4 + m√©tadonn√©es)
```

**Crit√®re de validation Phase 2 :**  
G√©n√©rer une vid√©o de 5 secondes, 720p, via l'API, en moins de 5 minutes, sur un GPU A100, **sans aucune connexion internet**.

---

## PHASE 3 ‚Äî ENTRA√éNER SES MOD√àLES (Semaines 6-10)

**Objectif :** Poss√©der chaque mod√®le. Ne plus d√©pendre des poids LTX-2/Lightricks d'origine.

---

### 3.1 Strat√©gie de fine-tuning ‚Äî Transformer SHDT

Le transformer est le c≈ìur du syst√®me. Le plan suit une progression curriculum (d√©j√† impl√©ment√© dans `curriculum_training.py`).

#### Phase 3.1.1 ‚Äî LoRA Fine-tuning (Semaine 6-7)

**Objectif :** Adapter le transformer LTX-2 aux cas d'usage AIPROD avec un co√ªt GPU minimal.

```yaml
Strat√©gie: LoRA
Rang LoRA: 32
Alpha: 32
Target modules: [q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj]
Mod√®le de base: models/ltx2_research/ltx-2-19b-dev-fp8.safetensors
Donn√©es: 500-1000 vid√©os courtes (5-10s) avec captions
Batch size: 1 (gradient accumulation 8)
Learning rate: 1e-5
GPU: 1√ó A100 80GB (ou 2√ó A100 40GB avec accelerate)
Epochs: 3-5
Dur√©e estim√©e: 48-72h
```

**Commande :**

```bash
python -m aiprod_trainer.trainer \
    --config configs/train/lora_phase1.yaml \
    --checkpoint models/ltx2_research/ltx-2-19b-dev-fp8.safetensors \
    --gemma-path models/text-encoder/gemma-3-1b \
    --output checkpoints/aiprod_lora_v1/ \
    --use-wandb false
```

**R√©sultat :** `checkpoints/aiprod_lora_v1/adapter_model.safetensors` (~50-200 MB)

#### Phase 3.1.2 ‚Äî Full Fine-tuning (Semaine 8-9)

**Objectif :** Entra√Æner le mod√®le complet pour devenir ind√©pendant des poids LTX-2 originaux.

```yaml
Strat√©gie: Full fine-tuning
Mod√®le de base: LTX-2 + LoRA v1 fusionn√©
Donn√©es: 5000+ vid√©os avec curriculum (r√©solution/dur√©e croissantes)
Curriculum:
  Phase 1: 256√ó256, 16 frames, lr=5e-6  (jours 1-3)
  Phase 2: 512√ó512, 32 frames, lr=3e-6  (jours 4-6)
  Phase 3: 768√ó768, 64 frames, lr=1e-6  (jours 7-9)
  Phase 4: 1024√ó576, 97 frames, lr=5e-7 (jours 10-12)
GPU: 4√ó A100 80GB (DDP via accelerate)
Gradient checkpointing: True
Mixed precision: bf16
Dur√©e estim√©e: 10-14 jours
```

**R√©sultat :** `checkpoints/aiprod_shdt_v1.safetensors` (~25 GB en bf16, ~12 GB en FP8)

#### Phase 3.1.3 ‚Äî Quantization propri√©taire (Semaine 10)

```bash
# Quantizer le mod√®le full en FP8 propri√©taire
python -m aiprod_trainer.quantization \
    --input checkpoints/aiprod_shdt_v1.safetensors \
    --output models/aiprod-sovereign/aiprod-shdt-v1-fp8.safetensors \
    --format fp8_e4m3
```

**R√©sultat final :** `models/aiprod-sovereign/aiprod-shdt-v1-fp8.safetensors` ‚Äî **mod√®le 100% propri√©taire**

---

### 3.2 Fine-tuning du Video VAE

**D√©j√† impl√©ment√©** dans `vae_trainer.py`. Le Haar Wavelet VAE (`HWVAEEncoder`/`HWVAEDecoder`) est propri√©taire.

```yaml
Donn√©es: M√™mes vid√©os que le transformer
Loss: Reconstruction L1 + Perceptual (VGG) + Spectral
GPU: 1√ó A100 80GB
Epochs: 50-100
Dur√©e: 3-5 jours
```

**R√©sultat :** `models/aiprod-sovereign/aiprod-hwvae-v1.safetensors`

---

### 3.3 Fine-tuning Audio VAE + Vocoder

L'audio codec NAC est impl√©ment√© dans `aiprod_core/model/audio_vae/codec.py`.

```yaml
Donn√©es: 1000+ clips audio (musique d'ambiance, voix, effets sonores)
Loss: Reconstruction + RVQ commitment loss + Spectral
GPU: 1√ó A100
Dur√©e: 2-3 jours
```

**R√©sultat :** `models/aiprod-sovereign/aiprod-audio-vae-v1.safetensors`

---

### 3.4 Entra√Ænement du TTS

L'architecture est compl√®te dans `aiprod_core/model/tts/` (Tacotron + HiFi-GAN + Prosody).

```yaml
Donn√©es: LJSpeech (13k clips, 24h, domaine public) + LibriTTS (585h, CC BY 4.0)
Phase 1: TextEncoder + MelDecoder sur LJSpeech (5 jours)
Phase 2: VocoderTTS (HiFi-GAN) sur mel spectrograms (3 jours)
Phase 3: ProsodyModeler fine-tuning (2 jours)
GPU: 1√ó A100
Total: 10 jours
```

**R√©sultat :** `models/aiprod-sovereign/aiprod-tts-v1.safetensors`

---

### 3.5 Entra√Ænement / Provisionnement du Text Encoder

**Option A (rapide ‚Äî Semaine 6) :** Utiliser Gemma-3-1B pr√©-entra√Æn√©, stock√© localement.  
Les poids Gemma sont Apache 2.0 ‚Äî pas de restriction de licence.

**Option B (souverain total ‚Äî Semaine 8+) :** Fine-tuner un text encoder plus petit sur des donn√©es de prompts vid√©o.

```yaml
Mod√®le de base: Gemma-3-1B (Apache 2.0)
Donn√©es: 100k paires (prompt, video caption)
M√©thode: LoRA sur les couches d'embedding ‚Üí projection vers l'espace latent SHDT
R√©sultat: models/aiprod-sovereign/aiprod-text-encoder-v1.safetensors
```

---

### 3.6 Structure finale du r√©pertoire mod√®les

```
models/
‚îú‚îÄ‚îÄ aiprod-sovereign/                      ‚Üê MOD√àLES PROPRI√âTAIRES
‚îÇ   ‚îú‚îÄ‚îÄ aiprod-shdt-v1-fp8.safetensors     ‚Üê Transformer diffusion (~12 GB)
‚îÇ   ‚îú‚îÄ‚îÄ aiprod-hwvae-v1.safetensors        ‚Üê Video VAE (~500 MB)
‚îÇ   ‚îú‚îÄ‚îÄ aiprod-audio-vae-v1.safetensors    ‚Üê Audio codec (~200 MB)
‚îÇ   ‚îú‚îÄ‚îÄ aiprod-tts-v1.safetensors          ‚Üê TTS complet (~300 MB)
‚îÇ   ‚îú‚îÄ‚îÄ aiprod-text-encoder-v1.safetensors ‚Üê Text encoder (~1 GB)
‚îÇ   ‚îú‚îÄ‚îÄ aiprod-upsampler-v1.safetensors    ‚Üê Spatial upsampler (~500 MB)
‚îÇ   ‚îú‚îÄ‚îÄ MANIFEST.json                      ‚Üê Versions + SHA-256 checksums
‚îÇ   ‚îî‚îÄ‚îÄ MODEL_CARD.md                      ‚Üê Documentation compl√®te
‚îÇ
‚îú‚îÄ‚îÄ ltx2_research/                         ‚Üê MOD√àLES DE BASE (Lightricks)
‚îÇ   ‚îú‚îÄ‚îÄ ltx-2-19b-dev-fp8.safetensors      ‚Üê Base pour fine-tuning
‚îÇ   ‚îî‚îÄ‚îÄ ltx-2-spatial-upscaler-x2-1.0.safetensors
‚îÇ
‚îú‚îÄ‚îÄ text-encoder/                          ‚Üê Pr√©-provisionn√© Phase 1
‚îÇ   ‚îî‚îÄ‚îÄ gemma-3-1b/
‚îÇ
‚îú‚îÄ‚îÄ scenarist/                             ‚Üê Pr√©-provisionn√© Phase 1
‚îÇ   ‚îî‚îÄ‚îÄ mistral-7b/
‚îÇ
‚îî‚îÄ‚îÄ qa/                                    ‚Üê QA s√©mantique
    ‚îî‚îÄ‚îÄ siglip-base/
```

**Crit√®re de validation Phase 3 :**
- G√©n√©rer une vid√©o de 10s, 1080p, **uniquement avec les mod√®les `aiprod-sovereign/`**
- Aucun fichier du dossier `ltx2_research/` n'est n√©cessaire
- FID score ‚â§ 1.5√ó celui du mod√®le LTX-2 de base
- Pas de r√©gression de qualit√© subjective (√©valuation humaine A/B)

---

## PHASE 4 ‚Äî VERROUILLER ET CERTIFIER (Semaines 11-12)

**Objectif :** Le syst√®me passe une due diligence technique. Reproductible. Document√©. Certifiable.

---

### 4.1 Reproductibilit√© compl√®te

#### Action 4.1.1 ‚Äî Fixer le determinism PyTorch

```python
# √Ä ajouter dans aiprod_core/utils.py ‚Üí seed_everything()
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
os.environ["CUBLAS_WORKSPACE_CONFIG"] = ":4096:8"
torch.use_deterministic_algorithms(True, warn_only=True)
```

#### Action 4.1.2 ‚Äî Lockfile complet

```
requirements.lock          ‚Üê pip freeze exact
cuda_version.txt           ‚Üê nvidia-smi output
docker_image_hash.txt      ‚Üê sha256 de l'image Docker
models/CHECKSUMS.sha256    ‚Üê hash de chaque fichier de poids
```

#### Action 4.1.3 ‚Äî Test de reproductibilit√©

```bash
# M√™me prompt, m√™me seed ‚Üí m√™me vid√©o (bit-exact ou perceptuellement identique)
python -m aiprod_pipelines.ti2vid_one_stage \
    --prompt "A cat walking" --seed 42 --output test_repro_1.mp4

python -m aiprod_pipelines.ti2vid_one_stage \
    --prompt "A cat walking" --seed 42 --output test_repro_2.mp4

# V√©rifier : PSNR(test_repro_1, test_repro_2) == inf (ou > 60 dB)
python scripts/compare_videos.py test_repro_1.mp4 test_repro_2.mp4
```

---

### 4.2 Tests automatis√©s complets

#### Action 4.2.1 ‚Äî Tests de souverainet√© automatis√©s

```
Nouveau fichier : tests/test_sovereignty.py
```

```python
"""Tests automatis√©s de souverainet√© ‚Äî ex√©cut√©s en CI sans r√©seau."""

import socket

class TestSovereignty:
    def test_no_network_calls_during_import(self):
        """Importer tout le projet ne d√©clenche aucun appel r√©seau."""
        original = socket.socket
        socket.socket = lambda *a, **k: (_ for _ in ()).throw(RuntimeError("Network blocked"))
        try:
            import aiprod_core
            import aiprod_pipelines
            import aiprod_trainer
        finally:
            socket.socket = original
    
    def test_all_models_present_locally(self):
        """Tous les mod√®les requis existent sur le filesystem."""
        assert Path("models/aiprod-sovereign/aiprod-shdt-v1-fp8.safetensors").exists()
        assert Path("models/aiprod-sovereign/aiprod-hwvae-v1.safetensors").exists()
        # ...
    
    def test_no_google_imports_in_core(self):
        """Aucun import Google dans aiprod_core."""
        for py_file in Path("packages/aiprod-core/src").rglob("*.py"):
            content = py_file.read_text()
            assert "google.generativeai" not in content
            assert "google.cloud" not in content
    
    def test_no_from_pretrained_without_local_only(self):
        """Chaque from_pretrained() a local_files_only=True."""
        for py_file in Path("packages").rglob("*.py"):
            content = py_file.read_text()
            if "from_pretrained(" in content:
                assert "local_files_only=True" in content or "local_files_only" in content
    
    def test_inference_offline(self):
        """G√©n√©rer une vid√©o sans connexion r√©seau."""
        # Bloque le r√©seau, charge le pipeline, g√©n√®re 1 frame
        ...
```

#### Action 4.2.2 ‚Äî CI/CD pipeline

```yaml
# .github/workflows/sovereignty-check.yml
name: Sovereignty Check
on: [push, pull_request]

jobs:
  sovereignty:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Install dependencies
        run: pip install -r requirements.txt  # Pas de packages cloud
      - name: Block network
        run: |
          # Iptables block all outbound except localhost
          sudo iptables -A OUTPUT -d 127.0.0.1 -j ACCEPT
          sudo iptables -A OUTPUT -j DROP
      - name: Run sovereignty tests
        run: pytest tests/test_sovereignty.py -v
```

---

### 4.3 Dockerfile souverain

#### Action 4.3.1 ‚Äî R√©√©crire `deploy/docker/Dockerfile.gpu`

```dockerfile
# === AIPROD SOVEREIGN GPU IMAGE ===
# Z√©ro d√©pendance SaaS. Z√©ro appel r√©seau en runtime.

FROM nvidia/cuda:12.4.0-devel-ubuntu22.04 AS builder

# ... (build steps identiques)

# CHANGEMENT CRITIQUE : ne PAS installer google-*, boto3, stripe, wandb
RUN pip install --no-cache-dir \
    torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 \
    --index-url https://download.pytorch.org/whl/cu124 && \
    pip install --no-cache-dir \
    -r /app/requirements.txt  # requirements.txt souverain (sans cloud deps)

# Copier les mod√®les dans l'image (build-time, pas runtime download)
COPY models/aiprod-sovereign/ /app/models/aiprod-sovereign/
COPY models/text-encoder/ /app/models/text-encoder/
COPY models/scenarist/ /app/models/scenarist/

# Variable d'environnement : config souveraine
ENV AIPROD_CONFIG=/app/config/AIPROD_V34_SOVEREIGN.json
ENV AIPROD_MODELS_DIR=/app/models/aiprod-sovereign
```

---

### 4.4 Documentation due diligence

#### Action 4.4.1 ‚Äî Cr√©er `docs/SOVEREIGNTY_CERTIFICATE.md`

Contenu :

```
1. Inventaire complet des mod√®les avec licences
   - SHDT Transformer : Propri√©taire (entra√Æn√© par AIPROD)
   - HW-VAE : Propri√©taire
   - NAC Audio Codec : Propri√©taire
   - TTS : Propri√©taire (entra√Æn√© sur LJSpeech CC0 + LibriTTS CC BY 4.0)
   - Text Encoder : Bas√© sur Gemma (Apache 2.0) + fine-tuning propri√©taire
   - QA CLIP : SigLIP (Apache 2.0, local inference)

2. Licences des d√©pendances open-source
   - PyTorch : BSD-3
   - transformers : Apache 2.0
   - safetensors : Apache 2.0
   - FastAPI : MIT
   - ffmpeg : LGPL 2.1

3. Inventaire des appels r√©seau
   - Runtime : Z√âRO
   - Build-time : PyPI (pip install), PyTorch wheel index
   - Provisionnement unique : HuggingFace Hub (pour mod√®les de base)

4. Test air-gapped r√©alis√© le [DATE]
   - Environnement : [description]
   - R√©sultat : [PASS/FAIL]
   - Vid√©os g√©n√©r√©es : [√©chantillons]
```

---

## PLANNING CONSOLID√â

```
SEMAINE  1  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  Phase 1.1 ‚Äî Pr√©-provisionner les mod√®les
SEMAINE  2  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  Phase 1.2-1.4 ‚Äî Couper Google, isoler cloud, figer deps
SEMAINE  3  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  Phase 2.1 ‚Äî GPU Worker (architecture + queue)
SEMAINE  4  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  Phase 2.1 ‚Äî GPU Worker (int√©gration pipeline)  
SEMAINE  5  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  Phase 2.2-2.4 ‚Äî Config V34, QA local, test end-to-end
SEMAINE  6  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  Phase 3.1.1 ‚Äî LoRA fine-tuning transformer
SEMAINE  7  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  Phase 3.1.1 ‚Äî LoRA validation + Phase 3.2 VAE
SEMAINE  8  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  Phase 3.1.2 ‚Äî Full fine-tuning (d√©but)
SEMAINE  9  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  Phase 3.1.2 ‚Äî Full fine-tuning (suite) + 3.3 Audio
SEMAINE 10  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  Phase 3.1.3 ‚Äî Quantization + 3.4 TTS + 3.5 Text Encoder
SEMAINE 11  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  Phase 4.1-4.2 ‚Äî Reproductibilit√© + tests souverainet√©
SEMAINE 12  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  Phase 4.3-4.4 ‚Äî Docker souverain + documentation certifi√©e
```

---

## BUDGET GPU ESTIM√â

| Phase | GPU n√©cessaire | Dur√©e | Co√ªt cloud estim√© |
|---|---|---|---|
| Phase 1 (couper les fils) | 0 GPU | 2 semaines | $0 |
| Phase 2 (connecter moteur) | 1√ó A100 40GB (tests) | 3 semaines | ~$500 |
| Phase 3 ‚Äî LoRA | 1√ó A100 80GB | 3 jours | ~$200 |
| Phase 3 ‚Äî Full fine-tune | 4√ó A100 80GB | 14 jours | ~$5,000 |
| Phase 3 ‚Äî VAE | 1√ó A100 80GB | 5 jours | ~$400 |
| Phase 3 ‚Äî Audio + TTS | 1√ó A100 40GB | 15 jours | ~$1,100 |
| Phase 3 ‚Äî Quantization | 1√ó A100 40GB | 1 jour | ~$70 |
| Phase 4 (tests) | 1√ó A100 40GB | 2 jours | ~$140 |
| **TOTAL** | | | **~$7,400** |

Avec GPU propre (achat A100 80GB ~$15,000) : amortissement en 2 cycles de fine-tuning.

---

## CRIT√àRES DE SUCC√àS FINAUX

| # | Crit√®re | Mesure | Seuil |
|---|---|---|---|
| 1 | **Air-gapped** | G√©n√©rer une vid√©o 10s sans r√©seau | ‚úÖ Fonctionne |
| 2 | **Z√©ro API externe** | `grep -r "google\|openai\|anthropic\|stripe" --include="*.py"` dans les imports actifs | 0 r√©sultat |
| 3 | **Mod√®les propri√©taires** | Tous les poids dans `models/aiprod-sovereign/` | 100% fichiers pr√©sents |
| 4 | **Pipeline r√©el** | POST `/v1/generate` retourne un `.mp4` valide | ffprobe OK |
| 5 | **Reproductible** | M√™me seed ‚Üí m√™me vid√©o | PSNR > 55 dB |
| 6 | **Requirements fig√©s** | `requirements.lock` existe et est valide | pip install OK |
| 7 | **Docker souverain** | Image Docker fonctionne air-gapped | ‚úÖ Fonctionne |
| 8 | **Tests CI** | `tests/test_sovereignty.py` passe √† 100% | 0 √©checs |
| 9 | **Qualit√© vid√©o** | FVD vs LTX-2 base | ‚â§ 1.3√ó baseline |
| 10 | **Documentation** | `SOVEREIGNTY_CERTIFICATE.md` complet | Auditable |

---

## SCORE CIBLE

| Crit√®re | Actuel | Cible Phase 4 |
|---|---|---|
| Souverainet√© r√©elle | 3/10 | **9/10** |
| Robustesse technique | 5/10 | **8/10** |
| Scalabilit√© GPU | 6/10 | **8/10** |
| Reproductibilit√© | 4/10 | **9/10** |
| Viabilit√© √©conomique | 3/10 | **7/10** |

**Verdict cible : üëâ 100% propri√©taire r√©el**

---

*Plan √©tabli le 2026-02-15.*  
*Premi√®re milestone : Phase 1 compl√©t√©e ‚Üí score souverainet√© 6/10.*  
*Milestone finale : Phase 4 compl√©t√©e ‚Üí score souverainet√© 9/10.*
