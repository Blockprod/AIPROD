# =============================================================================
# AIPROD GPU Worker Pool Deployment
# =============================================================================
# Runs inference on GPU node pool (A100/H100)
# Autoscales 0 â†’ 20 based on queue length
# Includes GPU liveness probe and VRAM health check
# =============================================================================
apiVersion: apps/v1
kind: Deployment
metadata:
  name: aiprod-gpu-worker
  namespace: aiprod-production
  labels:
    app.kubernetes.io/name: aiprod-gpu-worker
    app.kubernetes.io/component: gpu-inference
    app.kubernetes.io/part-of: aiprod-platform
spec:
  replicas: 2
  revisionHistoryLimit: 5
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app.kubernetes.io/name: aiprod-gpu-worker
  template:
    metadata:
      labels:
        app.kubernetes.io/name: aiprod-gpu-worker
        app.kubernetes.io/component: gpu-inference
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9090"
    spec:
      priorityClassName: aiprod-pro
      serviceAccountName: aiprod-worker-sa
      terminationGracePeriodSeconds: 300  # allow in-flight jobs to complete
      containers:
        - name: worker
          image: gcr.io/aiprod/gpu-worker:latest
          ports:
            - name: metrics
              containerPort: 9090
          env:
            - name: AIPROD_ENV
              value: production
            - name: NVIDIA_VISIBLE_DEVICES
              value: all
            - name: CUDA_MODULE_LOADING
              value: LAZY
            - name: REDIS_URL
              valueFrom:
                secretKeyRef:
                  name: aiprod-secrets
                  key: redis-url
            - name: MODEL_REGISTRY_URL
              value: "http://mlflow.aiprod-production:5000"
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: "http://otel-collector.aiprod-production:4317"
            - name: GCS_BUCKET
              value: "aiprod-videos-production"
          resources:
            requests:
              cpu: "4"
              memory: 32Gi
              nvidia.com/gpu: "1"
            limits:
              cpu: "8"
              memory: 64Gi
              nvidia.com/gpu: "1"
          volumeMounts:
            - name: model-cache
              mountPath: /app/models
            - name: dshm
              mountPath: /dev/shm
          # GPU-aware liveness probe
          livenessProbe:
            exec:
              command:
                - python
                - -c
                - |
                  import torch
                  assert torch.cuda.is_available(), 'GPU not available'
                  free, total = torch.cuda.mem_get_info(0)
                  assert free > 1024*1024*512, f'Low VRAM: {free/(1024**2):.0f}MB free'
                  print('GPU OK')
            initialDelaySeconds: 120
            periodSeconds: 60
            timeoutSeconds: 30
            failureThreshold: 3
          readinessProbe:
            exec:
              command:
                - python
                - -c
                - "import torch; assert torch.cuda.is_available()"
            initialDelaySeconds: 60
            periodSeconds: 30
            timeoutSeconds: 15
            failureThreshold: 2
      volumes:
        - name: model-cache
          persistentVolumeClaim:
            claimName: aiprod-model-cache
        - name: dshm
          emptyDir:
            medium: Memory
            sizeLimit: 16Gi
      nodeSelector:
        cloud.google.com/gke-accelerator: nvidia-tesla-a100
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
---
# GPU Worker HPA (scale on custom queue-length metric)
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: aiprod-gpu-worker-hpa
  namespace: aiprod-production
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: aiprod-gpu-worker
  minReplicas: 0  # scale to zero when idle (cost savings)
  maxReplicas: 20
  metrics:
    - type: Resource
      resource:
        name: nvidia.com/gpu
        target:
          type: Utilization
          averageUtilization: 75
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 60
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 30  # fast scale-up
      policies:
        - type: Pods
          value: 4
          periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 600  # slow scale-down (10 min)
      policies:
        - type: Pods
          value: 1
          periodSeconds: 300
---
# GPU Worker PDB
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: aiprod-gpu-worker-pdb
  namespace: aiprod-production
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: aiprod-gpu-worker
---
# Model cache PVC (shared across GPU workers)
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: aiprod-model-cache
  namespace: aiprod-production
spec:
  accessModes:
    - ReadOnlyMany
  storageClassName: standard-rwo
  resources:
    requests:
      storage: 200Gi
---
# ServiceAccount for GPU workers
apiVersion: v1
kind: ServiceAccount
metadata:
  name: aiprod-worker-sa
  namespace: aiprod-production
  annotations:
    iam.gke.io/gcp-service-account: aiprod-worker@aiprod.iam.gserviceaccount.com
---
# ServiceAccount for API gateway
apiVersion: v1
kind: ServiceAccount
metadata:
  name: aiprod-gateway-sa
  namespace: aiprod-production
  annotations:
    iam.gke.io/gcp-service-account: aiprod-gateway@aiprod.iam.gserviceaccount.com
