# üéØ AIPROD Local Development Guide

**Environment**: Windows 11 + GTX 1070 + Python 3.11.9 ‚úÖ

---

## üöÄ Getting Started (5 minutes)

### 1. Activate Virtual Environment
```powershell
. .venv_311\Scripts\Activate.ps1
```

### 2. Verify GPU
```powershell
python -c "import torch; print(f'GPU: {torch.cuda.is_available()}'); print(f'Device: {torch.cuda.get_device_name(0)}')"
```

Expected output:
```
GPU: True
Device: NVIDIA GeForce GTX 1070
```

### 3. Download Models (One-time setup)
```powershell
# Create models directory
mkdir -p models/aiprod2 models/gemma-3

# Download from HuggingFace:
# 1. aiprod-2-19b-dev-fp8.safetensors ‚Üí models/aiprod2/
# 2. aiprod-2-spatial-upscaler-x2-1.0.safetensors ‚Üí models/aiprod2/
# 3. gemma-3 model files ‚Üí models/gemma-3/

# Or use HuggingFace CLI:
huggingface-cli download Lightricks/LTX-2 --local-dir models/aiprod2
```

### 4. Run Quick Test
```powershell
python examples/quickstart.py --prompt "A cat sleeping on a sunny windowsill"
```

---

## üìö Available Pipelines

### TI2VidTwoStagesPipeline (Recommended for Quality)
```python
from aiprod_pipelines import TI2VidTwoStagesPipeline

pipeline = TI2VidTwoStagesPipeline(
    ckpt_dir="models/aiprod2",
    text_encoder_dir="models/gemma-3",
    fp8_transformer=True  # Important for GTX 1070!
)

video = pipeline(
    prompt="Your description here",
    height=480,
    width=832,
    num_frames=121
)

pipeline.save_video(video, "output.mp4", fps=24)
```

**Time**: 30-45 min | **Quality**: Excellent ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

---

### DistilledPipeline (Fast)
```python
from aiprod_pipelines import DistilledPipeline

pipeline = DistilledPipeline(
    ckpt_dir="models/aiprod2",
    text_encoder_dir="models/gemma-3"
)

video = pipeline(prompt="Quick video generation", seed=42)
pipeline.save_video(video, "output.mp4")
```

**Time**: 15-20 min | **Quality**: Good ‚≠ê‚≠ê‚≠ê‚≠ê

---

### TI2VidOneStagePipeline (Fastest)
```python
from aiprod_pipelines import TI2VidOneStagePipeline

pipeline = TI2VidOneStagePipeline(
    ckpt_dir="models/aiprod2",
    text_encoder_dir="models/gemma-3"
)

video = pipeline(prompt="Fast generation", seed=42)
pipeline.save_video(video, "output.mp4")
```

**Time**: 10-15 min | **Quality**: Acceptable ‚≠ê‚≠ê‚≠ê

---

## üñ•Ô∏è GPU Monitoring

### Monitor VRAM in Real-Time
```powershell
# Window 1: Run generation
python examples/quickstart.py

# Window 2: Monitor GPU
nvidia-smi -l 1  # Update every 1 second
```

Or in Python:
```python
import torch

# Check VRAM before generation
print(f"VRAM Used: {torch.cuda.memory_allocated() / 1e9:.2f} GB")
print(f"VRAM Total: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")

# Run your code

# Check VRAM after
print(f"VRAM Used: {torch.cuda.memory_allocated() / 1e9:.2f} GB")
```

### Optimize for GTX 1070 (8GB VRAM)

If you get **CUDA out of memory** errors:

```python
# 1. Enable FP8 (half precision)
pipeline = TI2VidTwoStagesPipeline(
    ckpt_dir="models/aiprod2",
    text_encoder_dir="models/gemma-3",
    fp8_transformer=True  # ‚Üê Always enable
)

# 2. Reduce resolution
video = pipeline(
    prompt="Your prompt",
    height=384,      # ‚Üê Smaller
    width=640,       # ‚Üê Smaller
    num_frames=60    # ‚Üê Fewer frames
)

# 3. Use DistilledPipeline (memory efficient)
from aiprod_pipelines import DistilledPipeline
pipeline = DistilledPipeline(...)

# 4. Clear cache between batches
torch.cuda.empty_cache()
```

---

## üé® Prompting Tips

### Good Prompts (Cinematic & Detailed)

‚ùå Bad: "A dog"  
‚úÖ Good: "A golden retriever running through a sunny field, shallow depth of field, cinematic lighting"

‚ùå Bad: "Car driving"  
‚úÖ Good: "Red sports car accelerating down a highway at sunset, motion blur, dynamic camera work"

### Structure
- **Action**: "Running, jumping, falling"
- **Subject**: "Golden retriever, person, spaceship"
- **Setting**: "In a forest, on the moon, in a city"
- **Style**: "Cinematic, photorealistic, 3D render"
- **Camera**: "Static, panning, zoom in"
- **Lighting**: "Golden hour, neon, dramatic shadows"

### Example (Excellent)
```
"A serene waterfall in a misty forest during golden hour, 
with light rays coming through the trees, water flowing 
smoothly, shot with a wide-angle lens, cinematic depth of field"
```

---

## üìä Development Workflow

### 1. Iterate Prompts
```python
prompts = [
    "A cat sleeping on a sunny windowsill",
    "A cat stretching and yawning in bright morning light",
    "A playful kitten pouncing on a toy"
]

for i, prompt in enumerate(prompts):
    video = pipeline(prompt=prompt, seed=42)
    pipeline.save_video(video, f"output_{i}.mp4")
```

### 2. Test Different Seeds
```python
# Same prompt, different seeds for variation
for seed in [42, 123, 456, 789]:
    video = pipeline(prompt="Your prompt", seed=seed)
    pipeline.save_video(video, f"output_seed_{seed}.mp4")
```

### 3. LoRA Fine-tuning (Advanced)
```python
# Train custom LoRA
from aiprod_trainer import LoraTrainer

trainer = LoraTrainer(
    model_ckpt="models/aiprod2/checkpoint.safetensors",
    dataset_path="datasets/my_videos",
    learning_rate=1e-4
)

trainer.train(num_epochs=10)
trainer.save_lora("output_lora.safetensors")

# Use custom LoRA
video = pipeline(
    prompt="Your prompt",
    lora_path="output_lora.safetensors",
    lora_scale=0.8
)
```

---

## üîß Common Issues & Solutions

### Issue 1: "CUDA out of memory"
```python
# Solution: Use FP8 + smaller resolution
pipeline = TI2VidTwoStagesPipeline(
    ckpt_dir="models/aiprod2",
    text_encoder_dir="models/gemma-3",
    fp8_transformer=True
)

video = pipeline(
    prompt="Your prompt",
    height=384,
    width=640,
    num_frames=60
)
```

### Issue 2: "Module not found: triton"
‚úÖ Already fixed in config (Windows shim installed)  
No action needed - kernel fusion just disabled

### Issue 3: "xFormers can't load C++/CUDA extensions"
‚úÖ Expected warning - version mismatch but functional  
No action needed - CPU fallback works

### Issue 4: Slow inference
**Solutions**:
1. ‚úÖ Use `DistilledPipeline` (2-3x faster)
2. ‚úÖ Reduce resolution (384x640 vs 480x832)
3. ‚úÖ Enable FP8 (already recommended)
4. ‚úÖ Upgrade to H100 (via HuggingFace Spaces - Phase 2)

---

## üìà Performance Benchmarks (GTX 1070)

| Pipeline | Resolution | Frames | Time | Quality |
|----------|---|---|---|---|
| TI2VidTwoStages | 480x832 | 121 | 30-45 min | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| DistilledPipeline | 480x832 | 121 | 15-20 min | ‚≠ê‚≠ê‚≠ê‚≠ê |
| TI2VidOneStage | 480x832 | 121 | 10-15 min | ‚≠ê‚≠ê‚≠ê |
| TI2VidTwoStages | 384x640 | 60 | 10-15 min | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| DistilledPipeline | 384x640 | 60 | 5-10 min | ‚≠ê‚≠ê‚≠ê‚≠ê |

---

## üéØ Next Steps

### Short-term (This Week)
- [ ] Download model checkpoints
- [ ] Run quickstart.py examples
- [ ] Test different prompts
- [ ] Experiment with pipelines

### Medium-term (Next Week)
- [ ] Fine-tune custom LoRA
- [ ] Build prompt library
- [ ] Optimize for your use case

### Long-term (Production)
- [ ] Move to Phase 2 (HuggingFace Spaces)
- [ ] Deploy Phase 3 (HF Inference API)
- [ ] Monetize via API

---

## üìö Useful Resources

| Resource | Link |
|----------|------|
| AIPROD Code | `packages/aiprod-pipelines/` |
| Examples | `examples/` |
| Configuration | `PHASE_1_GPU_CONFIG_COMPLETE.md` |
| Architecture | `COMPARAISON_AIPROD_vs_LTX2.md` |

---

## üö© Quick Commands

```powershell
# Activate venv
. .venv_311\Scripts\Activate.ps1

# Run quickstart
python examples/quickstart.py --prompt "Your prompt"

# Test GPU
python -c "import torch; print(torch.cuda.is_available())"

# Check VRAM
nvidia-smi

# List available pipelines
python -c "from aiprod_pipelines import *; print([x for x in dir() if 'Pipeline' in x])"

# Deactivate venv
deactivate
```

---

### üéâ Ready to Create!

Start with `examples/quickstart.py` and experiment with different prompts. GPU is ready! üöÄ

---

*Last updated: 10 February 2026*  
*Status: Ready for local development ‚úÖ*
