# Phase 2: Logging & ObservabilitÃ© - Guide de DÃ©marrage

**Status:** âœ… **PRÃŠT POUR DÃ‰PLOIEMENT** (FÃ©vrier 2, 2026)

## ğŸ“Š Vue d'ensemble Phase 2

Phase 2 ajoute l'observabilitÃ© complÃ¨te au pipeline AIPROD V33:

- **Prometheus:** Collecte des mÃ©triques (P50/P95/P99, coÃ»ts, erreurs)
- **Grafana:** 3 dashboards (Performance, CoÃ»ts, SLA)
- **AlertManager:** Alertes critiques vers Slack/PagerDuty
- **Jaeger:** Distributed tracing des appels Gemini
- **Structured Logging:** Logs JSON vers Google Cloud Logging
- **Custom Metrics:** 12 mÃ©triques Prometheus critiques

---

## ğŸš€ DÃ©marrage Rapide (Local)

### Ã‰tape 1: VÃ©rifier l'environnement Python

```bash
# VÃ©rifier .venv311 est activÃ©
(.venv311) PS C:\Users\averr\AIPROD_V33>

# VÃ©rifier les packages Phase 2
pip list | findstr prometheus jaeger
```

**RÃ©sultat attendu:**

```
jaeger-client 4.8.0
prometheus-client 0.24.1
prometheus-fastapi-instrumentator 7.1.0
google-cloud-logging 3.13.0
```

### Ã‰tape 2: DÃ©marrer Prometheus + Grafana + Jaeger

```bash
# (Optionnel) Nettoyer les anciens containers
docker-compose -f docker-compose.monitoring.yml down -v

# DÃ©marrer la stack d'observabilitÃ©
docker-compose -f docker-compose.monitoring.yml up -d

# VÃ©rifier l'Ã©tat
docker-compose -f docker-compose.monitoring.yml ps
```

**Outputs attendus:**

- `prometheus` â†’ http://localhost:9090
- `grafana` â†’ http://localhost:3000 (admin/admin)
- `alertmanager` â†’ http://localhost:9093
- `jaeger` â†’ http://localhost:16686

### Ã‰tape 3: DÃ©marrer l'API FastAPI

```bash
# Terminal 1 - DÃ©marrer l'API avec mÃ©triques Prometheus
python -m uvicorn src.api.main:app --reload --host 0.0.0.0 --port 8000

# Terminal 2 - VÃ©rifier les mÃ©triques
curl http://localhost:8000/metrics | head -20
```

---

## ğŸ“ˆ Dashboards Grafana

### AccÃ¨s Grafana

```
URL: http://localhost:3000
Login: admin
Password: admin
```

### 3 Dashboards ConfigurÃ©s

#### 1ï¸âƒ£ **Pipeline Performance**

**ClÃ©s Ã  surveiller:**

- Execution Rate (jobs/sec)
- P95 Latency (SLA: 300s)
- Active Jobs
- Success Rate by Preset

**Thresholds d'alerte:**

- ğŸŸ¡ Alerte SLA si P95 > 120s (warning)
- ğŸ”´ Critique si P95 > 300s (SLA breach)

#### 2ï¸âƒ£ **Cost Dashboard**

**ClÃ©s Ã  surveiller:**

- 24h Total Cost ($)
- Cost per Job (moyenne)
- Hourly Trend
- % du Daily Budget ($2000)

**Thresholds d'alerte:**

- ğŸŸ¡ Alerte si > 70% du budget journalier
- ğŸ”´ Critique si > 90% du budget

#### 3ï¸âƒ£ **SLA & Error Tracking**

**ClÃ©s Ã  surveiller:**

- Success Rate (Target: 99.5%)
- Render Failures Timeline
- QA Gate Acceptance Rate
- HTTP Error Rate by Endpoint

**Thresholds d'alerte:**

- ğŸŸ¡ Alerte si success < 95%
- ğŸ”´ Critique si success < 90%
- ğŸ”´ Critique si render failures > 5/10min

---

## ğŸš¨ AlertManager Configuration

### Alertes Critiques (10 dÃ©finies)

1. **PipelineHighErrorRate** â†’ Slack #critical-alerts + PagerDuty
2. **PipelineCostThresholdExceeded** â†’ Slack #sla-alerts
3. **PipelineLatencySLABreach** â†’ Slack #critical-alerts
4. **PubSubQueueDepthHigh** â†’ Slack #infra-alerts
5. **AIAgentTimeouts** â†’ Slack #critical-alerts + PagerDuty
6. **QAGateHighRejectionRate** â†’ Slack #sla-alerts
7. **VideoRenderFailures** â†’ Slack #critical-alerts
8. **DatabaseLatencyHigh** â†’ Slack #infra-alerts
9. **PrometheusDown** â†’ Slack #critical-alerts + PagerDuty
10. **ActiveJobsSpike** â†’ Slack #sla-alerts

### Configuration des Notifications

**Fichier:** `config/alertmanager.yml`

```yaml
# Slack Webhook (env var requis)
SLACK_WEBHOOK_URL=https://hooks.slack.com/services/...

# PagerDuty (env var requis)
PAGERDUTY_SERVICE_KEY=...
```

---

## ğŸ“Š MÃ©triques Prometheus ExposÃ©es

### 12 Custom Metrics

| MÃ©trique                       | Type      | Labels                     | UtilitÃ©               |
| ------------------------------ | --------- | -------------------------- | --------------------- |
| `pipeline_duration_seconds`    | Histogram | status, preset, agent_type | P50/P95/P99 latency   |
| `pipeline_cost_dollars`        | Histogram | status, backend, preset    | CoÃ»t par job          |
| `ai_agent_calls_total`         | Counter   | agent_type, status, model  | Gemini usage tracking |
| `render_failures_total`        | Counter   | reason, backend, stage     | Error categorization  |
| `user_jobs_completed_total`    | Counter   | preset, quality_tier       | Success rate          |
| `pipeline_active_jobs`         | Gauge     | status, preset             | Real-time job count   |
| `pubsub_queue_depth`           | Gauge     | topic                      | Queue latency         |
| `ai_agent_latency_seconds`     | Summary   | agent_type, model          | Quantile latency      |
| `qa_gate_acceptance_total`     | Counter   | result, stage              | QA acceptance rate    |
| `video_output_size_bytes`      | Histogram | preset, duration           | Output size dist      |
| `db_operation_latency_seconds` | Histogram | operation, table           | DB latency tracking   |
| `api_response_time_seconds`    | Histogram | method, endpoint, status   | API performance       |

---

## ğŸ” Structured Logging vers Google Cloud Logging

### Activation (Configuration Requise)

**Fichier:** `src/utils/structured_logging.py`

```python
from src.utils.structured_logging import (
    logger,
    set_correlation_id,
    set_trace_id,
    set_user_id
)

# Dans chaque request handler
@app.post("/api/pipeline/execute")
async def execute_pipeline(request: PipelineRequest):
    # GÃ©nÃ©rer correlation ID unique pour le job
    cid = set_correlation_id()
    tid = set_trace_id()

    logger.info(
        "Pipeline execution started",
        job_id=job_id,
        preset=request.preset,
        cost_estimate=estimated_cost
    )
```

**Variables d'environnement requises:**

```bash
GOOGLE_CLOUD_PROJECT=aiprod-v33
```

---

## ğŸ”„ Jaeger Distributed Tracing

### Configuration des Traces

**Clients actuellement tracÃ©es:**

- `src/agents/semantic_qa.py` â†’ Appels Gemini
- `src/agents/visual_translator.py` â†’ Appels Gemini
- `src/api/main.py` â†’ Endpoints HTTP

### Jaeger UI

```
URL: http://localhost:16686
Services: aiprod-api, prometheus, grafana
```

**Traces utiles:**

1. Pipeline end-to-end trace (input â†’ agents â†’ output)
2. Gemini API call latency distribution
3. Database operations timeline

---

## ğŸ“ Runbooks (Ã€ ComplÃ©ter en P2.4)

CrÃ©er des runbooks pour chaque alerte:

```
docs/runbooks/
â”œâ”€â”€ high-error-rate.md          # Triage erreurs pipeline
â”œâ”€â”€ cost-threshold.md            # Optimisation coÃ»ts
â”œâ”€â”€ latency-sla.md              # AmÃ©lioration performance
â”œâ”€â”€ pubsub-queue.md             # Queue troubleshooting
â”œâ”€â”€ agent-timeout.md            # Agent debugging
â”œâ”€â”€ qa-gate-rejection.md        # Quality troubleshooting
â”œâ”€â”€ render-failures.md          # Render debugging
â”œâ”€â”€ db-latency.md               # Database optimization
â”œâ”€â”€ prometheus-down.md          # Monitoring recovery
â””â”€â”€ active-jobs-spike.md        # Load investigation
```

---

## ğŸ§ª Test de l'ObservabilitÃ© (Local)

### 1. Tester les MÃ©triques Prometheus

```bash
# VÃ©rifier metrics endpoint
curl http://localhost:8000/metrics

# Chercher une mÃ©trique spÃ©cifique
curl -s http://localhost:8000/metrics | grep pipeline_duration

# VÃ©rifier Prometheus scrape
curl http://localhost:9090/api/v1/query?query=pipeline_duration_seconds_bucket
```

### 2. DÃ©clencher une Alerte Test

```bash
# Simuler une exÃ©cution pipeline (gÃªnÃ¨re des mÃ©triques)
curl -X POST http://localhost:8000/api/pipeline/execute \
  -H "Content-Type: application/json" \
  -d '{
    "content": "Test video content",
    "preset": "quick_social",
    "duration_sec": 30
  }'
```

### 3. VÃ©rifier Grafana Dashboards

1. Aller Ã  http://localhost:3000
2. Chercher "AIPROD V33 - Pipeline Performance"
3. VÃ©rifier que les mÃ©triques s'affichent (dÃ©lai ~10-15 secondes)

### 4. VÃ©rifier AlertManager

```bash
# Lister les alertes actuelles
curl http://localhost:9093/api/v1/alerts

# VÃ©rifier la config
curl http://localhost:9093/api/v1/alerts/groups
```

---

## ğŸ³ Mode Conteneur (Production)

### DÃ©ployer sur Cloud Run

```bash
# 1. Build l'image avec observabilitÃ©
docker build -t aiprod-v33:latest .

# 2. Push sur GCR
docker tag aiprod-v33:latest gcr.io/aiprod-prod/aiprod-v33:latest
docker push gcr.io/aiprod-prod/aiprod-v33:latest

# 3. Deploy sur Cloud Run avec Prometheus
gcloud run deploy aiprod-v33 \
  --image=gcr.io/aiprod-prod/aiprod-v33:latest \
  --port=8000 \
  --cpu=4 \
  --memory=16Gi \
  --timeout=3600 \
  --env=GOOGLE_CLOUD_PROJECT=aiprod-prod \
  --env=SLACK_WEBHOOK_URL=... \
  --env=PAGERDUTY_SERVICE_KEY=...
```

### Prometheus Remote Write vers Google Cloud Monitoring

Ajouter Ã  `config/prometheus.yml`:

```yaml
remote_write:
  - url: https://monitoring.googleapis.com/api/v1/projects/aiprod-prod/timeSeries
    write_relabel_configs:
      - source_labels: [__name__]
        regex: "pipeline_.*|ai_agent_.*|render_.*"
        action: keep
```

---

## ğŸ“‹ Checklist ComplÃ©tion Phase 2

- âœ… Prometheus + Grafana setup (local)
- âœ… 3 Dashboards Grafana crÃ©Ã©s
- âœ… 12 Custom metrics dÃ©finies
- âœ… AlertManager rules (10 alertes)
- âœ… Jaeger tracing configurÃ©
- âœ… Structured logging (JSON â†’ Cloud Logging)
- â³ Tests d'intÃ©gration (P2.4)
- â³ Runbooks documentation (P2.4)
- â³ Production deployment (P2.4)
- â³ Slack/PagerDuty integration (P2.4)

---

## ğŸ¯ Prochaines Ã‰tapes (P2.4)

1. **Tester localement** - ExÃ©cuter les dashboards avec donnÃ©es rÃ©elles
2. **CrÃ©er runbooks** - Doc pour chaque alerte critique
3. **IntÃ©grer Slack** - Connecter webhook AlertManager
4. **Deployer production** - Cloud Run + Cloud Monitoring
5. **On-call setup** - PagerDuty escalation policies

---

## ğŸ“ Support

**Logs dÃ©taillÃ©s:** `logs/aiprod.log`
**Grafana URL:** http://localhost:3000 (admin/admin)
**Prometheus URL:** http://localhost:9090
**Jaeger URL:** http://localhost:16686
