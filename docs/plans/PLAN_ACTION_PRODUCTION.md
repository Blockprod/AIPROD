# üìã PLAN D'ACTION PRODUCTION ‚Äî AIPROD_V33

**Document** : Plan d'action complet pour rendre AIPROD_V33 pr√™t pour la production  
**Date** : 2 f√©vrier 2026  
**Scope** : Bas√© sur AUDIT_COMPLET + AUDIT_TECHNIQUE  
**Dur√©e totale estim√©e** : 6-8 semaines  
**√âquipe requise** : 2-3 backend engineers + 1 DevOps/SRE

---

## üìå Executive Summary

AIPROD_V33 est une plateforme beta bien architectur√©e mais **non productible** en l'√©tat. Les audits identifient **4 risques critiques** et **6 am√©liorations majeures**. Ce plan priorise les actions urgentes (24-48h), puis les blocages structurels (1-2 semaines), et enfin les optimisations (mois 1-2).

| Phase              | Dur√©e   | Risques lev√©s | Effort |
| ------------------ | ------- | ------------- | ------ |
| **0 - Critique**   | 24-48h  | S√©curit√©      | 10j    |
| **1 - Fondation**  | 1-2 sem | Scalabilit√©   | 15j    |
| **2 - Robustesse** | 2-3 sem | Fiabilit√©     | 10j    |
| **3 - Production** | 1 mois  | Op√©rationnel  | 15j    |

---

# üî¥ PHASE 0 ‚Äî CRITIQUES (24-48 HEURES)

## P0.1 S√©curit√© : Secrets expos√©s

### Issue

- ‚úã **S√©v√©rit√©** : CRITIQUE
- üìç **Fichier** : `.env`
- üîì **Exposition** : Cl√©s API Gemini, Runway, Datadog, credentials GCP
- ‚ö†Ô∏è **Impact** : Compromission imm√©diate si versionn√©

### Actions

#### P0.1.1 Audit & R√©vocation (2h)

```bash
# 1. V√©rifier historique git pour expositions
git log --oneline --all -- .env
git log -p --all -S "AIzaSy" | head -100  # Scanner cl√©s Gemini
git log -p --all -S "key_" | head -100     # Scanner cl√©s Runway

# 2. Rev√©rifier le .env actuel
cat .env | grep -E "API_KEY|SECRET|CREDENTIALS"

# 3. R√©voquer les cl√©s compromises
# ‚Üí GCP Console : Disable service account keys
# ‚Üí Gemini API Console : Regenerate API keys
# ‚Üí Runway ML : Regenerate API keys
# ‚Üí Datadog : Regenerate API keys
```

**Checklist** :

- [ ] Toutes les cl√©s git history supprim√©es (git filter-branch ou GitHub Tool)
- [ ] Cl√©s GCP r√©voqu√©es dans IAM
- [ ] Cl√©s Gemini r√©voqu√©es
- [ ] Cl√©s Runway r√©voqu√©es
- [ ] Cl√©s Datadog r√©voqu√©es
- [ ] `.env` ajout√© √† `.gitignore` (si pas d√©j√† fait)

#### P0.1.2 Migration Secret Manager (3h)

```bash
# 1. Cr√©er Secret Manager GCP
gcloud secrets create GEMINI_API_KEY --replication-policy="automatic"
gcloud secrets create RUNWAY_API_KEY --replication-policy="automatic"
gcloud secrets create DATADOG_API_KEY --replication-policy="automatic"
gcloud secrets create GCS_BUCKET_NAME --replication-policy="automatic"

# 2. Stocker les nouvelles cl√©s
echo "AIzaSy_NEW_KEY" | gcloud secrets versions add GEMINI_API_KEY --data-file=-

# 3. Cr√©er .env.example (sans valeurs)
cat > .env.example << 'EOF'
# R√©cup√©r√©s depuis GCP Secret Manager
GEMINI_API_KEY=<from GCP Secret Manager>
RUNWAY_API_KEY=<from GCP Secret Manager>
DATADOG_API_KEY=<from GCP Secret Manager>
GCS_BUCKET_NAME=<from GCP Secret Manager>
EOF
git add .env.example && git commit -m "chore: add .env.example template"
```

#### P0.1.3 Code : Charger secrets runtime (2h)

Fichier : `src/config/secrets.py` (cr√©er)

```python
import os
from google.cloud import secretmanager

def get_secret(secret_id: str) -> str:
    """Charge un secret depuis GCP Secret Manager."""
    project_id = os.getenv("GCP_PROJECT_ID", "aiprod-484120")
    client = secretmanager.SecretManagerServiceClient()
    name = f"projects/{project_id}/secrets/{secret_id}/versions/latest"
    response = client.access_secret_version(request={"name": name})
    return response.payload.data.decode("UTF-8")

def load_secrets():
    """Charge tous les secrets au d√©marrage."""
    os.environ["GEMINI_API_KEY"] = get_secret("GEMINI_API_KEY")
    os.environ["RUNWAY_API_KEY"] = get_secret("RUNWAY_API_KEY")
    os.environ["DATADOG_API_KEY"] = get_secret("DATADOG_API_KEY")
    os.environ["GCS_BUCKET_NAME"] = get_secret("GCS_BUCKET_NAME")
```

Fichier : `src/api/main.py` (modifi√©)

```python
# Au d√©marrage de l'app
from src.config.secrets import load_secrets

@app.on_event("startup")
async def startup_event():
    load_secrets()
    logger.info("Secrets charg√©s depuis GCP Secret Manager")
```

**Checklist** :

- [ ] Secret Manager GCP configur√©
- [ ] 5 secrets cr√©√©s + migr√©s
- [ ] Code de chargement impl√©ment√©
- [ ] Tests avec mock secrets
- [ ] `.env` supprim√© du repo
- [ ] Documentation mise √† jour

---

## P0.2 S√©curit√© : Pas d'authentification API

### Issue

- ‚úã **S√©v√©rit√©** : CRITIQUE
- üìç **Endpoints affect√©s** : `/pipeline/run`, `/metrics`, `/alerts`, `/icc/data`
- ‚ö†Ô∏è **Impact** : DDOS, modification d'√©tat, fuite de donn√©es

### Actions

#### P0.2.1 Ajouter JWT + Firebase Auth (6h)

**Installation**

```bash
pip install firebase-admin python-jose
```

Fichier : `src/auth/firebase_auth.py` (cr√©er)

```python
import os
import firebase_admin
from firebase_admin import credentials, auth

# Initialiser Firebase (credentials via Secret Manager)
if not firebase_admin.get_app():
    cred = credentials.Certificate({
        "project_id": os.getenv("GCP_PROJECT_ID"),
        # ... charger depuis Secret Manager
    })
    firebase_admin.initialize_app(cred)

def verify_token(token: str) -> dict:
    """V√©rifie un token JWT Firebase."""
    try:
        decoded = auth.verify_id_token(token)
        return decoded
    except Exception as e:
        raise ValueError(f"Token invalide: {e}")

def verify_api_key(api_key: str) -> bool:
    """V√©rifie une API key (alternative JWT)."""
    # Impl√©menter v√©rification avec base de donn√©es
    # Pour MVP : liste blanche de cl√©s
    pass
```

Fichier : `src/api/auth_middleware.py` (cr√©er)

```python
from fastapi import Depends, HTTPException, status
from fastapi.security import HTTPBearer, HTTPAuthCredentials
from src.auth.firebase_auth import verify_token

security = HTTPBearer()

async def verify_request(credentials: HTTPAuthCredentials = Depends(security)):
    """Middleware pour v√©rifier authentification."""
    token = credentials.credentials
    try:
        user = verify_token(token)
        return user
    except Exception:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Token invalide",
        )
```

Fichier : `src/api/main.py` (modifi√©)

```python
from src.api.auth_middleware import verify_request

# Routes prot√©g√©es
@app.post("/pipeline/run")
async def run_pipeline(
    request: PipelineRequest,
    user = Depends(verify_request)  # ‚úÖ Auth requis
):
    logger.info(f"Pipeline lanc√© par {user['uid']}")
    # ... reste du code

# Routes publiques
@app.get("/health")
async def health():
    return {"status": "ok"}

# Routes metrics (optionnellement prot√©g√©es)
@app.get("/metrics")
async def get_metrics(user = Depends(verify_request)):
    return metrics_collector.get_internal_metrics()
```

**Checklist** :

- [ ] Firebase project cr√©√© + configur√©
- [ ] Firebase Auth middleware impl√©ment√©
- [ ] JWT verification test√©
- [ ] API prot√©g√©e `/pipeline/run`
- [ ] Documentation auth mise √† jour
- [ ] Client frontend peut obtenir token

---

## P0.3 S√©curit√© : Passwords/configs en dur

### Issue

- üìç **Fichiers** : `docker-compose.yml` (Grafana password = `admin`)
- ‚ö†Ô∏è **Impact** : Acc√®s non autoris√© Grafana, risque donn√©es

### Actions

#### P0.3.1 Grafana (1h)

```yaml
# docker-compose.yml (modifi√©)
services:
  grafana:
    image: grafana/grafana:latest
    ports:
      - "3030:3000"
    volumes:
      - ./config/grafana:/var/lib/grafana
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD} # ‚úÖ Depuis env/secret
      - GF_SERVER_ROOT_URL=https://grafana.aiprod.prod # ‚úÖ HTTPS
      - GF_SECURITY_COOKIE_SECURE=true # ‚úÖ Secure cookies
      - GF_SECURITY_COOKIE_HTTPONLY=true
    restart: unless-stopped
```

**Checklist** :

- [ ] Password Grafana chang√© (> 16 chars, mixed case)
- [ ] Stock√© en Secret Manager
- [ ] `docker-compose.yml` mis √† jour
- [ ] HTTPS forc√© Grafana
- [ ] Acc√®s IP restreint

---

## P0.4 S√©curit√© : Audit log manquant

### Actions (2h)

Fichier : `src/security/audit_logger.py` (cr√©er)

```python
import logging
import json
from datetime import datetime
from enum import Enum

class AuditAction(str, Enum):
    API_CALL = "API_CALL"
    PIPELINE_START = "PIPELINE_START"
    PIPELINE_COMPLETE = "PIPELINE_COMPLETE"
    SECRET_ACCESS = "SECRET_ACCESS"
    ERROR = "ERROR"

audit_logger = logging.getLogger("audit")

def log_audit(action: AuditAction, user_id: str, details: dict):
    """Enregistre une action pour audit trail."""
    audit_logger.info(json.dumps({
        "timestamp": datetime.utcnow().isoformat(),
        "action": action.value,
        "user_id": user_id,
        "details": details,
    }))
```

Fichier : `src/api/main.py` (modifi√©)

```python
from src.security.audit_logger import log_audit, AuditAction

@app.post("/pipeline/run")
async def run_pipeline(request: PipelineRequest, user = Depends(verify_request)):
    log_audit(
        AuditAction.PIPELINE_START,
        user["uid"],
        {"content": request.content[:50], "preset": request.preset}
    )
    # ... reste du code
```

**Checklist** :

- [ ] Audit logger impl√©ment√©
- [ ] Tous les endpoints critiques loggent
- [ ] Logs export√©s vers Cloud Logging

---

## üìã P0 Summary

| Action                  | Dur√©e   | Owner      | Status |
| ----------------------- | ------- | ---------- | ------ |
| P0.1 - Secrets expos√©s  | 7h      | Backend    | [ ]    |
| P0.2 - Auth API         | 6h      | Backend    | [ ]    |
| P0.3 - Grafana password | 1h      | DevOps     | [ ]    |
| P0.4 - Audit logging    | 2h      | Backend    | [ ]    |
| **Total P0**            | **16h** | **24-48h** | [ ]    |

**Validation** :

```bash
# V√©rifier tous les secrets sont hors du code
git grep "AIzaSy" -- ':!.env.example'  # Doit √™tre vide
git grep "key_" -- ':!.env.example'

# V√©rifier API requiert auth
curl http://localhost:8000/pipeline/run  # Doit retourner 401
curl -H "Authorization: Bearer TOKEN" http://localhost:8000/pipeline/run  # Doit passer auth

# V√©rifier Grafana password chang√©
curl http://localhost:3030 -u admin:admin  # Doit √©chouer
```

---

# üü† PHASE 1 ‚Äî FONDATION (1-2 SEMAINES)

## P1.1 Persistance : Remplacer JobManager RAM par PostgreSQL

### Issue

- üìç **Actuel** : `src/api/icc_manager.py:JobManager._jobs` en Dict RAM
- ‚ö†Ô∏è **Impact** : Perte d'√©tat au red√©marrage, pas de multi-instance

### Actions

#### P1.1.1 Schema PostgreSQL (2h)

Fichier : `migrations/001_create_jobs_table.sql` (cr√©er)

```sql
CREATE TABLE IF NOT EXISTS jobs (
    id VARCHAR(36) PRIMARY KEY,
    content TEXT NOT NULL,
    preset VARCHAR(50),
    state VARCHAR(50) NOT NULL,
    priority VARCHAR(20) DEFAULT 'low',
    lang VARCHAR(10) DEFAULT 'en',
    brand_id VARCHAR(255),

    production_manifest JSONB,
    consistency_markers JSONB,
    cost_estimate JSONB,
    render_result JSONB,
    qa_report JSONB,

    approved BOOLEAN DEFAULT FALSE,
    approval_timestamp TIMESTAMP,
    edits_history JSONB DEFAULT '[]'::jsonb,

    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,

    INDEX idx_state (state),
    INDEX idx_created_at (created_at),
    INDEX idx_brand_id (brand_id)
);

CREATE TABLE IF NOT EXISTS job_events (
    id SERIAL PRIMARY KEY,
    job_id VARCHAR(36) REFERENCES jobs(id),
    event_type VARCHAR(50),
    event_data JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,

    INDEX idx_job_id (job_id),
    INDEX idx_created_at (created_at)
);
```

#### P1.1.2 Refactor JobManager (8h)

Fichier : `src/persistence/db.py` (cr√©er)

```python
import os
from sqlalchemy import create_engine
from sqlalchemy.pool import NullPool
from sqlalchemy.orm import sessionmaker

DATABASE_URL = os.getenv(
    "DATABASE_URL",
    "postgresql://aiprod:password@localhost:5432/aiprod_v33"
)

engine = create_engine(
    DATABASE_URL,
    echo=False,
    poolclass=NullPool,  # Cloud Run serverless
)

SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()
```

Fichier : `src/persistence/models.py` (cr√©er)

```python
from sqlalchemy import Column, String, Boolean, DateTime, JSON, func
from sqlalchemy.ext.declarative import declarative_base
from datetime import datetime

Base = declarative_base()

class JobModel(Base):
    __tablename__ = "jobs"

    id = Column(String(36), primary_key=True)
    content = Column(String, nullable=False)
    preset = Column(String(50))
    state = Column(String(50), nullable=False)
    priority = Column(String(20), default="low")
    lang = Column(String(10), default="en")
    brand_id = Column(String(255))

    production_manifest = Column(JSON)
    consistency_markers = Column(JSON)
    cost_estimate = Column(JSON)
    render_result = Column(JSON)
    qa_report = Column(JSON)

    approved = Column(Boolean, default=False)
    approval_timestamp = Column(DateTime)
    edits_history = Column(JSON, default=[])

    created_at = Column(DateTime, default=func.now())
    updated_at = Column(DateTime, default=func.now(), onupdate=func.now())
```

Fichier : `src/api/icc_manager.py` (refactoris√©)

```python
from sqlalchemy.orm import Session
from src.persistence.models import JobModel

class JobManager:
    def __init__(self, db: Session):
        self.db = db

    async def create_job(self, content: str, **kwargs) -> JobModel:
        """Cr√©e un job en base de donn√©es."""
        job = JobModel(
            id=str(uuid.uuid4())[:8],
            content=content,
            state=JobState.CREATED,
            **kwargs
        )
        self.db.add(job)
        self.db.commit()
        self.db.refresh(job)
        return job

    async def get_job(self, job_id: str) -> JobModel:
        return self.db.query(JobModel).filter(JobModel.id == job_id).first()

    async def update_job_state(self, job_id: str, new_state: JobState):
        job = self.db.query(JobModel).filter(JobModel.id == job_id).first()
        if job:
            job.state = new_state
            job.updated_at = datetime.utcnow()
            self.db.commit()
            self.db.refresh(job)
        return job
```

Fichier : `src/api/main.py` (modifi√©)

```python
from sqlalchemy.orm import Session
from src.persistence.db import get_db
from src.api.icc_manager import JobManager

@app.post("/pipeline/run")
async def run_pipeline(
    request: PipelineRequest,
    user = Depends(verify_request),
    db: Session = Depends(get_db)
):
    job_manager = JobManager(db)
    job = await job_manager.create_job(
        content=request.content,
        priority=request.priority,
        lang=request.lang
    )
    # ... ex√©cute pipeline avec db persistence
```

**Checklist** :

- [ ] PostgreSQL deployed (Cloud SQL recommand√©)
- [ ] Migrations ex√©cut√©es
- [ ] Models SQLAlchemy cr√©√©s
- [ ] JobManager refactoris√© + test√©
- [ ] Connection pooling configur√©
- [ ] Backups automatiques configur√©s

---

## P1.2 Distribution : Ajouter queue Pub/Sub

### Issue

- üìç **Actuel** : Rendu synchrone bloque l'API
- ‚ö†Ô∏è **Impact** : Pas de scalabilit√©, timeout sur requ√™tes longues

### Actions

#### P1.2.1 Setup Pub/Sub GCP (2h)

```bash
# Cr√©er topics
gcloud pubsub topics create aiprod-pipeline-requests
gcloud pubsub topics create aiprod-pipeline-results

# Cr√©er subscriptions
gcloud pubsub subscriptions create aiprod-render-worker \
  --topic aiprod-pipeline-requests \
  --push-endpoint=https://render-worker.aiprod.prod/process

gcloud pubsub subscriptions create aiprod-results-processor \
  --topic aiprod-pipeline-results
```

#### P1.2.2 Refactor API pour Pub/Sub (6h)

Fichier : `src/queue/publisher.py` (cr√©er)

```python
from google.cloud import pubsub_v1
import json

publisher = pubsub_v1.PublisherClient()
PROJECT_ID = "aiprod-484120"
TOPIC_ID = "aiprod-pipeline-requests"

async def publish_job(job_id: str, pipeline_request: dict):
    """Publie un job dans la queue."""
    topic_path = publisher.topic_path(PROJECT_ID, TOPIC_ID)
    message_json = json.dumps({
        "job_id": job_id,
        "request": pipeline_request,
    })
    future = publisher.publish(topic_path, message_json.encode('utf-8'))
    message_id = future.result()
    logger.info(f"Job {job_id} publi√©: {message_id}")
    return message_id
```

Fichier : `src/api/main.py` (modifi√©)

```python
from src.queue.publisher import publish_job

@app.post("/pipeline/run")
async def run_pipeline(
    request: PipelineRequest,
    user = Depends(verify_request),
    db: Session = Depends(get_db)
):
    # Cr√©er job en DB
    job_manager = JobManager(db)
    job = await job_manager.create_job(content=request.content, ...)

    # Publier dans queue
    await publish_job(job.id, request.model_dump())

    # Retourner imm√©diatement au client
    return {
        "job_id": job.id,
        "status": "queued",
        "message": "Pipeline lanc√©, vous recevrez une notification √† la completion"
    }
```

#### P1.2.3 Worker Pub/Sub (8h)

Fichier : `src/workers/render_worker.py` (cr√©er)

```python
from google.cloud import pubsub_v1
from src.orchestrator.state_machine import StateMachine
from src.persistence.db import SessionLocal

subscriber = pubsub_v1.SubscriberClient()
PROJECT_ID = "aiprod-484120"
SUBSCRIPTION_ID = "aiprod-render-worker"

subscription_path = subscriber.subscription_path(PROJECT_ID, SUBSCRIPTION_ID)

def process_job(message):
    """Traite un job de la queue."""
    try:
        data = json.loads(message.data.decode('utf-8'))
        job_id = data["job_id"]
        request = data["request"]

        # Charger job depuis DB
        db = SessionLocal()
        job = db.query(JobModel).filter(JobModel.id == job_id).first()

        if not job:
            logger.error(f"Job {job_id} not found")
            message.ack()
            return

        # Ex√©cuter pipeline
        state_machine = StateMachine()
        result = asyncio.run(state_machine.run(request))

        # Mettre √† jour job
        job.state = PipelineState.DELIVERED
        job.render_result = result
        db.commit()

        logger.info(f"Job {job_id} completed")
        message.ack()  # Acknowledge apr√®s succ√®s

    except Exception as e:
        logger.error(f"Job processing failed: {e}")
        message.nack()  # Requeue si erreur

def listen():
    """√âcoute la queue ind√©finiment."""
    streaming_pull_future = subscriber.subscribe(subscription_path, process_job)
    logger.info(f"Listening on {subscription_path}")

    try:
        streaming_pull_future.result()
    except KeyboardInterrupt:
        streaming_pull_future.cancel()
```

Fichier : `scripts/run_render_worker.py` (cr√©er)

```python
#!/usr/bin/env python
import asyncio
from src.workers.render_worker import listen

if __name__ == "__main__":
    asyncio.run(listen())
```

**Checklist** :

- [ ] Pub/Sub topics cr√©√©s
- [ ] API refactoris√©e (async return)
- [ ] Worker impl√©ment√© + test√©
- [ ] Worker d√©ploy√© en Cloud Run (job)
- [ ] Queue monitoring en place
- [ ] Dead letter queue configur√©e

---

## P1.3 Remplacer mocks par impl√©mentations r√©elles

### Issue

- üìç **Mocks** : `SemanticQA`, `VisualTranslator`, `GCP Integrator`
- ‚ö†Ô∏è **Impact** : R√©sultats non fiables

### Actions

#### P1.3.1 SemanticQA ‚Üí LLM r√©el (4h)

Fichier : `src/agents/semantic_qa.py` (refactoris√©)

```python
import asyncio
import google.generativeai as genai
from src.utils.monitoring import logger

class SemanticQA:
    def __init__(self):
        self.model = genai.GenerativeModel("gemini-1.5-pro-vision")

    async def run(self, outputs: Dict[str, Any]) -> Dict[str, Any]:
        """Valide s√©mantiquement les outputs avec LLM r√©el."""
        try:
            render_output = outputs.get("render", {})
            video_url = render_output.get("video_url", "")

            prompt = f"""Analysez cette vid√©o g√©n√©r√©e par IA:
            - URL: {video_url}
            - Prompt original: {outputs.get('prompt', '')}

            √âvaluez:
            1. Qualit√© visuelle (0-1)
            2. Pertinence au prompt (0-1)
            3. Artefacts/erreurs?
            4. Note globale (0-1)

            Format JSON: {{"quality": X, "relevance": X, "artifacts": [], "score": X}}
            """

            response = await self.model.generate_content_async(prompt)
            result = json.loads(response.text)

            logger.info(f"SemanticQA: quality={result['quality']}, relevance={result['relevance']}")

            return {
                "semantic_valid": result["score"] >= 0.7,
                "quality_score": result["quality"],
                "relevance_score": result["relevance"],
                "artifacts": result.get("artifacts", []),
                "overall_score": result["score"],
            }
        except Exception as e:
            logger.error(f"SemanticQA error: {e}")
            return {"semantic_valid": False, "error": str(e)}
```

#### P1.3.2 VisualTranslator ‚Üí Gemini real (3h)

Similar to SemanticQA, mais avec prompts de traduction visuelle.

#### P1.3.3 GCP Integrator ‚Üí Cloud Storage real (4h)

Fichier : `src/agents/gcp_services_integrator.py` (refactoris√©)

```python
from google.cloud import storage
import asyncio

class GoogleCloudServicesIntegrator:
    def __init__(self):
        self.storage_client = storage.Client()
        self.bucket_name = os.getenv("GCS_BUCKET_NAME")

    async def run(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Upload r√©el vers GCS."""
        try:
            bucket = self.storage_client.bucket(self.bucket_name)

            # Upload video
            video_blob = bucket.blob(f"videos/{inputs['job_id']}/output.mp4")
            video_blob.upload_from_filename(inputs["video_path"])

            # Generate signed URL (7 jours)
            video_url = video_blob.generate_signed_url(
                version="v4",
                expiration=datetime.timedelta(days=7),
                method="GET"
            )

            return {
                "status": "uploaded",
                "video_url": video_url,
                "storage_path": f"gs://{self.bucket_name}/videos/{inputs['job_id']}/output.mp4",
            }
        except Exception as e:
            logger.error(f"GCP upload error: {e}")
            return {"status": "error", "error": str(e)}
```

**Checklist** :

- [ ] SemanticQA impl√©ment√© + test√©
- [ ] VisualTranslator impl√©ment√© + test√©
- [ ] GCP Integrator impl√©ment√© + test√©
- [ ] Error handling pour API failures
- [ ] Fallback strat√©gies en place

---

## P1.4 CI/CD Pipeline

### Issue

- üìç **Actuellement** : Pas de CI/CD
- ‚ö†Ô∏è **Impact** : D√©ploiements manuels, risqu√©

### Actions

#### P1.4.1 GitHub Actions (4h)

Fichier : `.github/workflows/test.yml` (cr√©er)

```yaml
name: Tests

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main, develop]

jobs:
  test:
    runs-on: ubuntu-latest
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: postgres
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install pytest pytest-cov black ruff

      - name: Lint with Ruff
        run: ruff check src/

      - name: Format check with Black
        run: black --check src/

      - name: Run tests
        env:
          DATABASE_URL: postgresql://postgres:postgres@localhost/aiprod_test
        run: pytest tests/ -v --cov=src --cov-report=xml

      - name: Upload coverage
        uses: codecov/codecov-action@v3
```

Fichier : `.github/workflows/deploy.yml` (cr√©er)

```yaml
name: Deploy to Production

on:
  push:
    branches: [main]
    tags: [v*]

jobs:
  deploy:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v3

      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v1
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v1

      - name: Build and push Docker image
        run: |
          gcloud builds submit \
            --tag gcr.io/${{ secrets.GCP_PROJECT }}/aiprod-api:${{ github.sha }} \
            --substitutions _IMAGE_NAME=aiprod-api

      - name: Deploy to Cloud Run
        run: |
          gcloud run deploy aiprod-api \
            --image gcr.io/${{ secrets.GCP_PROJECT }}/aiprod-api:${{ github.sha }} \
            --platform managed \
            --region us-central1 \
            --allow-unauthenticated=false
```

**Checklist** :

- [ ] GitHub Actions configur√©
- [ ] Tests ex√©cut√©s √† chaque PR
- [ ] Coverage rapport√©
- [ ] Linting en place (Ruff, Black)
- [ ] D√©ploiement automatique main ‚Üí prod

---

## üìã P1 Summary

| Action               | Dur√©e   | Owner          | Status |
| -------------------- | ------- | -------------- | ------ |
| P1.1 - PostgreSQL    | 10h     | Backend        | [ ]    |
| P1.2 - Pub/Sub queue | 16h     | Backend/DevOps | [ ]    |
| P1.3 - Mocks ‚Üí r√©els | 11h     | Backend        | [ ]    |
| P1.4 - CI/CD         | 4h      | DevOps         | [ ]    |
| **Total P1**         | **41h** | **1-2 sem**    | [ ]    |

---

# üü° PHASE 2 ‚Äî ROBUSTESSE (2-3 SEMAINES)

## P2.1 Logging & Observabilit√©

### Actions (8h)

#### P2.1.1 Structured JSON Logging

Fichier : `src/config/logging_config.py` (cr√©er)

```python
import logging
import json
import sys
from pythonjsonlogger import jsonlogger

def setup_logging():
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)

    # Stdout (pour Cloud Logging)
    handler = logging.StreamHandler(sys.stdout)
    formatter = jsonlogger.JsonFormatter()
    handler.setFormatter(formatter)
    logger.addHandler(handler)

    return logger
```

#### P2.1.2 OpenTelemetry Tracing

```bash
pip install opentelemetry-api opentelemetry-sdk opentelemetry-exporter-gcp-trace
```

Fichier : `src/config/tracing.py` (cr√©er)

```python
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.exporter.gcp_trace import CloudTraceExporter

exporter = CloudTraceExporter()
trace.set_tracer_provider(TracerProvider(resource_attributes={"service.name": "aiprod"}))
trace.get_tracer_provider().add_span_processor(
    opentelemetry.sdk.trace.export.SimpleSpanProcessor(exporter)
)

tracer = trace.get_tracer(__name__)
```

**Checklist** :

- [ ] JSON logs configur√©s
- [ ] Cloud Logging re√ßoit les logs
- [ ] Tracing export√© en Cloud Trace
- [ ] Latency monitored

---

## P2.2 Tests & Couverture

### Actions (10h)

- [ ] Ajouter tests security (injection, auth bypass)
- [ ] Tests de charge (k6 ou locust)
- [ ] Tests de concurrence (multi-jobs)
- [ ] Coverage > 80% pour core logic
- [ ] Mutation testing (pit)

---

## P2.3 Monitoring & Alerting

### Actions (6h)

Fichier : `deployments/monitoring.yaml` (mise √† jour)

```yaml
alertPolicy:
  displayName: "AIPROD High Latency"
  conditions:
    - displayName: "Pipeline latency > 60s"
      conditionThreshold:
        filter: |
          resource.type="cloud_run_revision"
          metric.type="custom.googleapis.com/pipeline_latency_ms"
        comparison: COMPARISON_GT
        thresholdValue: 60000
        duration: 300s
  notificationChannels:
    - projects/aiprod-484120/notificationChannels/123456 # Slack/Email
```

**Checklist** :

- [ ] Alertes PagerDuty/Slack
- [ ] Seuils SLO d√©finis
- [ ] Dashboard Grafana cr√©√©
- [ ] On-call rotation en place

---

## P2.4 Documentation Op√©rationnel

### Actions (6h)

Cr√©er fichiers :

- `docs/RUNBOOK.md` - Incident response
- `docs/DEPLOYMENT.md` - Deploy procedure
- `docs/TROUBLESHOOTING.md` - Common issues
- `docs/SECURITY.md` - Security practices

**Checklist** :

- [ ] Runbook complet
- [ ] Deployment checklist
- [ ] Incident templates
- [ ] On-call guide

---

## üìã P2 Summary

| Action          | Dur√©e   | Owner       | Status |
| --------------- | ------- | ----------- | ------ |
| P2.1 - Logging  | 8h      | Backend     | [ ]    |
| P2.2 - Tests    | 10h     | Backend     | [ ]    |
| P2.3 - Alerting | 6h      | DevOps      | [ ]    |
| P2.4 - Docs     | 6h      | Tech Lead   | [ ]    |
| **Total P2**    | **30h** | **2-3 sem** | [ ]    |

---

# üü¢ PHASE 3 ‚Äî PRODUCTION (1 MOIS)

## P3.1 Infrastructure as Code (Terraform)

### Actions (12h)

Fichier : `terraform/main.tf` (cr√©er)

```hcl
terraform {
  required_version = ">= 1.0"
  required_providers {
    google = {
      source  = "hashicorp/google"
      version = "~> 5.0"
    }
  }
}

provider "google" {
  project = var.gcp_project_id
  region  = var.gcp_region
}

# Cloud SQL (PostgreSQL)
resource "google_sql_database_instance" "aiprod" {
  name             = "aiprod-db"
  database_version = "POSTGRES_15"
  region           = var.gcp_region

  settings {
    tier              = "db-custom-2-8192"
    availability_type = "REGIONAL"
    backup_configuration {
      enabled = true
      backup_retention_settings {
        retained_backups = 30
      }
    }
  }
}

# Cloud Run
resource "google_cloud_run_service" "aiprod_api" {
  name     = "aiprod-api"
  location = var.gcp_region

  template {
    spec {
      containers {
        image = var.docker_image
        env {
          name  = "DATABASE_URL"
          value = google_sql_database_instance.aiprod.connection_name
        }
      }
      service_account_name = google_service_account.aiprod.email
    }
  }
}

# Cloud Monitoring Uptime Check
resource "google_monitoring_uptime_check_config" "aiprod" {
  display_name = "AIPROD API Uptime"
  timeout      = "10s"
  period       = "60s"

  http_check {
    path = "/health"
    port = 443
  }

  selected_regions = ["USA", "EUROPE", "ASIA_PACIFIC"]
}
```

**Checklist** :

- [ ] Terraform code √©crit + document√©
- [ ] √âtat Terraform distant (GCS)
- [ ] D√©ploiement via Terraform approuv√©
- [ ] Staging environnement en Terraform

---

## P3.2 Scalabilit√© & Performance

### Actions (10h)

- [ ] Horizontal scaling tested (5+ instances)
- [ ] Database connection pooling (PgBouncer)
- [ ] Redis cache layer
- [ ] CDN pour assets vid√©o (Cloud CDN)
- [ ] Load testing reproductible (k6 + CI)

---

## P3.3 Disaster Recovery

### Actions (8h)

- [ ] Backup strategy document√©e
- [ ] RTO/RPO d√©fini
- [ ] Failover procedure tested
- [ ] Secrets rotation policy
- [ ] Data retention policy

---

## P3.4 Cost Optimization

### Actions (6h)

- [ ] Cost analysis (Recommender)
- [ ] Reserved instances pour Cloud SQL
- [ ] Spot VMs pour workers
- [ ] Budget alerts >150% budget

---

## üìã P3 Summary

| Action             | Dur√©e   | Owner           | Status |
| ------------------ | ------- | --------------- | ------ |
| P3.1 - Terraform   | 12h     | DevOps          | [ ]    |
| P3.2 - Scalabilit√© | 10h     | Backend/DevOps  | [ ]    |
| P3.3 - DR          | 8h      | DevOps/Security | [ ]    |
| P3.4 - Costs       | 6h      | DevOps/PM       | [ ]    |
| **Total P3**       | **36h** | **1 mois**      | [ ]    |

---

# üìä TIMELINE GLOBALE

```
JOUR 1-2        | P0 - S√©curit√© (16h)       [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà]
SEMAINE 1-2     | P1 - Fondation (41h)      [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà]
SEMAINE 3-4     | P2 - Robustesse (30h)     [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà]
SEMAINE 5-8     | P3 - Production (36h)     [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà]

TOTAL: 16h + 41h + 30h + 36h = 123 jours-homme (~6-8 semaines, 2-3 personnes)
```

---

# ‚úÖ VALIDATION & SIGN-OFF

## Pre-Production Checklist

### S√©curit√©

- [ ] Secrets hors du code (Secret Manager)
- [ ] Auth API (Firebase/JWT)
- [ ] HTTPS forc√©
- [ ] Audit trail complet
- [ ] Vuln√©rabilit√© scan pass√© (Trivy)

### Scalabilit√©

- [ ] PostgreSQL avec replicas
- [ ] Pub/Sub queue en place
- [ ] Horizontal scaling test√©
- [ ] Load test pass√© (1000 req/s)
- [ ] Database queries optimis√©es

### Fiabilit√©

- [ ] Tests coverage > 80%
- [ ] Mocks remplac√©s
- [ ] Error handling normalis√©
- [ ] Retry logic test√©
- [ ] Health checks en place

### Op√©rationnel

- [ ] CI/CD pipeline automatis√©
- [ ] Logs JSON en Cloud Logging
- [ ] Monitoring + alerting actifs
- [ ] Runbook √©crit + test√©
- [ ] On-call rotation d√©finie

### Co√ªt

- [ ] Budget mensuel < $500
- [ ] Cost alerts configur√©es
- [ ] Reserved instances r√©serv√©es

## Sign-off

| Role          | Responsable     | Sign-off |
| ------------- | --------------- | -------- |
| **Architect** | Backend Lead    | [ ]      |
| **DevOps**    | Cloud Engineer  | [ ]      |
| **Security**  | Security Lead   | [ ]      |
| **PM**        | Product Manager | [ ]      |

---

# üéØ POST-PROD (SEMAINES 9-12)

## Monitoring & Optimization

- Analyser m√©triques r√©elles
- Tuning performance database
- Co√ªts r√©els vs pr√©visions
- User feedback collection
- Roadmap Phase 4

---

**Document sign√© le** : 2 f√©vrier 2026  
**Version** : 1.0  
**Prochain review** : Post-P0 (48h)
