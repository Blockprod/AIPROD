# AUDIT ARCHITECTURAL ‚Äî AIPROD

**Date :** 14 f√©vrier 2026  
**Auditeur :** Principal AI Systems Architect & Production Infrastructure Auditor  
**P√©rim√®tre :** Analyse compl√®te du code source ‚Äî `C:\Users\averr\AIPROD`  
**M√©thode :** Lecture int√©grale de chaque fichier source, config, script, notebook et checkpoint  
**Tol√©rance √† l'illusion technologique : z√©ro**

---

## 1. Nature r√©elle du syst√®me

### Description exacte inf√©r√©e depuis le code

AIPROD est constitu√© de **trois packages Python** dans un monorepo :

| Package | Lignes de code | Nature r√©elle |
|---------|---------------|---------------|
| `aiprod-core` | ~7 500 (moteur) + ~1 870 (prototype) | **Fork renomm√© de LTX-Video 2.0 (Lightricks)** avec remplacement syst√©matique des cha√Ænes "LTX" ‚Üí "AIPROD" |
| `aiprod-pipelines` | ~63 800 | Couche d'orchestration/pipeline autour du moteur LTX-Video. ~2 000 lignes de pipelines r√©els, ~62 000 lignes d'infrastructure (SaaS, tensor parallelism, distributed LoRA, etc.) largement non connect√©es |
| `aiprod-trainer` | ~5 000+ | **Toolkit de fine-tuning LoRA** sur le mod√®le LTX-Video 2.0 pr√©-entra√Æn√©. Ne fait PAS d'entra√Ænement from scratch |

### Mod√®les r√©ellement propri√©taires ?

üî¥ **NON.**

Le moteur de production (`aiprod_core/model/`) est un **fork direct de LTX-Video 2.0 de Lightricks** :

- Architecture transformer : 48 couches, 32 t√™tes d'attention, head_dim=128, cross_attention_dim=4096 ‚Äî **identique √† LTX-Video 2.0**
- Video VAE : compression spatiale 32√ó, temporelle 8√ó, contrainte `1 + 8k` frames ‚Äî **identique √† LTX-Video 2.0**
- Audio VAE + Vocoder HiFi-GAN ‚Äî **branche audio de LTX-Video 2.0**
- Scheduler flow-matching avec `flux_time_shift` ‚Äî **LTX-Video 2.0**
- Text encoder : Google Gemma 3 (hidden_size=3840, 48 layers, vocab=262 208) ‚Äî mod√®le open-source tiers
- Renommages : `LTXRopeType` ‚Üí `AIPRODRopeType`, `LTXModel` ‚Üí `AIPRODModel`, etc.
- Classes `PixArtAlphaTextProjection` et `PixArtAlphaCombinedTimestepSizeEmbeddings` conserv√©es avec URLs GitHub d'origine
- **Aucune notice de copyright, aucun header de licence, aucune attribution Lightricks/LTX-Video** dans le code source

Un second code "prototype" existe dans `src/models/backbone.py` (768-D, attention+CNN interleaved) : original mais **toy model** non connect√© au moteur de production, inutilisable pour la g√©n√©ration vid√©o.

### Stack r√©ellement internalis√©e ?

üî¥ **NON.**

| Composant | R√©alit√© |
|-----------|---------|
| Mod√®le diffusion vid√©o | Fork LTX-Video 2.0 (Lightricks) |
| Text encoder | Google Gemma 3 (open-source) |
| Audio VAE + Vocoder | Fork LTX-Video 2.0 |
| Direction cr√©ative | Google Gemini 1.5 Pro (API externe) |
| Rendu vid√©o SaaS | Google Veo-3, Runway Gen3, Replicate WAN-2.5 (APIs externes) |
| Captioning vid√©o | Qwen2.5-Omni-7B ou Gemini Flash (mod√®les tiers) |
| D√©ploiement | Google Cloud Run (pas de GPU dans le Dockerfile) |

### Autonomie r√©elle ou d√©pendance cach√©e ?

üî¥ **D√©pendance totale.** Le syst√®me d√©pend de :

1. **Lightricks** pour l'architecture et les poids du mod√®le fondamental
2. **Google** pour le text encoder (Gemma 3), la direction cr√©ative (Gemini), et potentiellement le rendu (Veo-3)
3. **Runway / Replicate** comme backends de rendu alternatifs dans le pipeline SaaS
4. **HuggingFace** pour l'h√©bergement et le t√©l√©chargement des mod√®les

### Coh√©rence globale

Le projet pr√©sente une **dualit√© architecturale non r√©solue** :

1. **Pipeline local GPU** : inference text-to-video via les 5 pipelines (`TI2VidOneStage`, `TI2VidTwoStages`, `Distilled`, `ICLora`, `KeyframeInterpolation`) ‚Äî code fonctionnel et de bonne qualit√©
2. **Pipeline SaaS/orchestrateur** : state machine 11 √©tats avec API Cloud Run, qui d√©l√®gue le rendu √† des APIs externes (Veo-3, Runway, etc.) ‚Äî **aucun GPU dans le Dockerfile de d√©ploiement**

Ces deux architectures coexistent sans lien clair. Le SaaS ne d√©ploie pas le mod√®le local.

---

## 2. Mod√®les propri√©taires

### Types de mod√®les utilis√©s

| Type | Mod√®le r√©el | Propri√©taire ? |
|------|-------------|---------------|
| LLM interne | Aucun ‚Äî utilise Gemini 1.5 Pro (API Google) | üî¥ Non |
| Diffusion vid√©o | Fork LTX-Video 2.0 (~1.9B params) | üî¥ Non ‚Äî fork renomm√© |
| VAE vid√©o | Fork LTX-Video 2.0 VAE (3D causal) | üî¥ Non |
| Audio VAE | Fork LTX-Video 2.0 audio branch | üî¥ Non |
| Vocoder | HiFi-GAN (fork LTX-Video) | üî¥ Non |
| TTS | Inexistant | üî¥ N/A |
| Musique | Inexistant | üî¥ N/A |
| Coh√©rence inter-sc√®nes | Inexistant en production | üî¥ N/A |
| Text encoder | Google Gemma 3 (4B params) | üî¥ Non ‚Äî open-source tiers |

### Architecture des mod√®les

- **Transformer diffusion** : Flow-matching avec AdaLN (PixArt-Alpha), RoPE 3D, attention multi-modale (vid√©o + audio + cross-modal), STG guidance
- **Video VAE** : Encoder/decoder causal 3D, blocs ResNet, attention spatiale, compression 32√ó32√ó8
- **Audio VAE** : Encoder/decoder mel-spectrogram, convolutions causales, normalisation par canal
- **Vocoder** : Style HiFi-GAN pour reconstruction waveform
- **Tout issu de LTX-Video 2.0**

### Taille estim√©e des mod√®les

| Mod√®le | Param√®tres estim√©s |
|--------|-------------------|
| Transformer (AIPRODModel) | ~1.9B (48 layers √ó 32 heads √ó 128 head_dim) |
| Video VAE | ~150-300M |
| Audio VAE + Vocoder | ~50-100M |
| Gemma 3 Text Encoder | ~4B |
| **Total inference** | **~6-7B param√®tres** |

### Pipeline d'entra√Ænement

üî¥ **Aucun entra√Ænement from scratch n'est r√©alis√© ni r√©alisable dans l'√©tat actuel.**

**Ce qui existe :**

- Un toolkit de **fine-tuning LoRA** (`aiprod-trainer`) pour adapter le mod√®le LTX-Video 2.0 pr√©-entra√Æn√©
- LoRA rank 16-32 sur attention vid√©o, audio, et cross-modale
- Configs Accelerate (DDP, FSDP) pour multi-GPU
- Pipeline de pr√©-processing : captioning ‚Üí embeddings ‚Üí latents
- Validation avec sampling p√©riodique
- Tracking via Weights & Biases

**Ce qui manque :**

- üî¥ Aucun entra√Ænement du transformer fondamental (1.9B params)
- üî¥ Aucun entra√Ænement du VAE vid√©o
- üî¥ Aucun entra√Ænement de l'audio VAE
- üî¥ Aucun entra√Ænement du text encoder (Gemma 3 gel√©)
- üî¥ Aucun dataset propri√©taire identifiable (r√©pertoires vides : `models/aiprod2/`, `models/gemma-3/`, `models/pretrained/`)
- üî¥ Le prototype `train.py` dans `aiprod-core/src/training/` entra√Æne un **toy model** non connect√© au moteur de production (backbone 768-D + VAE simpliste avec donn√©es synth√©tiques rectangles)
- üî¥ Le seul checkpoint existant (`PHASE_1_SIMPLE_epoch_0.pt`, 152 MB) est celui du toy model ‚Äî un mod√®le LTX-Video r√©el ferait 18-40 GB

**Dataset r√©el ou th√©orique ?**

üî¥ Th√©orique. Les scripts de preprocessing (`process_videos.py`, `process_captions.py`, `split_scenes.py`) existent et sont fonctionnels, mais :
- Le r√©pertoire `datasets/` est dans `.gitignore` (aucune donn√©e commitable)
- Aucun log de training dans `logs/` (r√©pertoire vide)
- Aucune trace d'ex√©cution r√©elle du pipeline de training

**Co√ªt estim√© d'entra√Ænement from scratch :**

| Phase | Estimation |
|-------|-----------|
| Pr√©-training transformer 1.9B (video + audio) | 500-2 000 heures A100 (~$1-4M) |
| Pr√©-training Video VAE | 100-500 heures A100 (~$200K-1M) |
| Pr√©-training Audio VAE + Vocoder | 50-200 heures A100 (~$100-400K) |
| Fine-tuning LoRA (ce qui est couvert) | 10-50 heures A100 (~$20-100K) |
| **Total from scratch** | **$1.3-5.4M minimum** |

üî¥ Le fine-tuning LoRA (seule capacit√© actuelle) ne constitue pas un "mod√®le propri√©taire". Il produit un adaptateur de quelques dizaines de MB sur un mod√®le tiers de 19 GB.

---

## 3. Pipeline d'inf√©rence

### Optimisation GPU

| Optimisation | Impl√©ment√©e ? |
|-------------|--------------|
| FP8 quantization transformer | ‚úÖ Oui (via `optimum-quanto`) |
| Tiled VAE decoding | ‚úÖ Oui (blending trap√©zo√Ødal) |
| Multi-backend attention (PyTorch, xFormers, FlashAttention3) | ‚úÖ Oui |
| Batching | üü† Non ‚Äî inf√©rence single-request |
| Distillation | üü† Pipeline `distilled.py` existe mais utilise des sigma pr√©-calcul√©s, pas de vrai mod√®le distill√© |
| torch.compile | ‚úÖ Configs Accelerate avec backend Inductor |

### Latence estim√©e

- Vid√©o 30s @ 25fps (750 frames) sur A100 80GB : **~5-15 minutes** (estimation bas√©e sur LTX-Video 2.0 benchmarks)
- Vid√©o 30s @ 25fps sur GTX 1070 : commentaire dans le code dit "15-45 minutes"
- üü† Aucun benchmark r√©el ex√©cut√© (logs vides)

### Stabilit√© des g√©n√©rations

- CFG + STG guidance impl√©ment√©s
- APG (Adaptive Projected Guidance) impl√©ment√©
- Multi-modal guidance (audio/vid√©o isolation) impl√©ment√©
- üü° Pas de m√©triques de qualit√© collect√©es en production

### Maintien coh√©rence multi-sc√®nes

üî¥ **Non impl√©ment√©.** Aucun module de coh√©rence inter-sc√®nes fonctionnel. Le module `multimodal_coherence/` dans inference (~2 400 lignes) est structurel uniquement ‚Äî algorithmes simplifi√©s, pas connect√© au pipeline r√©el.

---

## 4. Orchestration distribu√©e

### State machine r√©elle

‚úÖ Oui ‚Äî `Orchestrator` avec 11 √©tats (INIT ‚Üí ANALYSIS ‚Üí CREATIVE_DIRECTION ‚Üí VISUAL_TRANSLATION ‚Üí FINANCIAL_OPTIMIZATION ‚Üí RENDER_EXECUTION ‚Üí QA_TECHNICAL ‚Üí QA_SEMANTIC ‚Üí FINALIZE + FAST_TRACK + ERROR). Checkpoint/resume JSON fonctionnel.

### Gestion multi-GPU

| Capacit√© | √âtat |
|----------|------|
| Training multi-GPU DDP | ‚úÖ Configs Accelerate fonctionnelles |
| Training FSDP | ‚úÖ Config avec wrapping `BasicAVTransformerBlock` |
| Inference multi-GPU | üî¥ Non impl√©ment√© |
| tensor_parallelism (inference) | üî¥ Module existe (~1 756 lignes) mais **non connect√©** ‚Äî structures de donn√©es et configs seulement |

### Multi-node training

üü† Th√©orique via Accelerate ‚Äî configs DDP supportent multi-process mais aucune trace de training multi-node.

### Scheduler interne / Priorit√©s SaaS

Le module `multi_tenant_saas/` (~2 471 lignes) impl√©mente :
- Tenant management, auth JWT, RBAC
- Billing, rate limiting, job scheduling
- Feature flags, monitoring

üî¥ **Aucune int√©gration backend r√©elle.** Toutes les classes ont de la logique interne mais pas de connexion √† une base de donn√©es, un message broker, ou un syst√®me de queue.

### Isolation des jobs clients

üî¥ Non impl√©ment√©e en production. Le module SaaS est structurel.

### Retry logic

‚úÖ Impl√©ment√©e dans l'adaptateur de rendu (`render.py`) avec cha√Æne de fallback Veo-3 ‚Üí Runway ‚Üí Replicate.

### Reproductibilit√© d√©terministe

üü† Seeds configurables dans les pipelines mais pas de garantie de reproductibilit√© cross-GPU/cross-version.

---

## 5. Pipeline Vid√©o interne

### Mod√®le vid√©o r√©ellement comp√©titif ?

Le mod√®le est **identique √† LTX-Video 2.0**, donc :
- ‚úÖ Comp√©titif au niveau de LTX-Video 2.0 (mod√®le open-source de bonne qualit√©)
- üî¥ Pas un avantage comp√©titif ‚Äî n'importe qui peut utiliser LTX-Video 2.0

### R√©solution native

- Stage 1 : 512√ó768 (demi-r√©solution)
- Stage 2 (upsampling) : 1024√ó1536
- D√©fini dans `constants.py`

### Frame interpolation

‚úÖ Pipeline `KeyframeInterpolationPipeline` impl√©ment√© ‚Äî interpolation entre keyframes via diffusion.

### Coh√©rence temporelle

‚úÖ Assur√©e par le RoPE 3D (positional encoding spatial + temporel) du transformer. H√©rit√© de LTX-Video 2.0.

### Contr√¥le cam√©ra r√©el ou simul√© ?

üî¥ **Non impl√©ment√©.** Aucun syst√®me de contr√¥le cam√©ra (pas de ControlNet cam√©ra, pas de param√®tres de mouvement cam√©ra dans les pipelines).

### Risque d'artefacts

üü† Standard pour un mod√®le diffusion 1.9B ‚Äî artefacts possibles sur les mouvements complexes, les mains, les visages. M√™me profil de risque que LTX-Video 2.0 vanilla.

---

## 6. Pipeline Audio interne

### Mod√®le TTS propri√©taire ?

üî¥ **Inexistant.** Aucun mod√®le TTS dans le code. L'audio VAE g√©n√®re de l'audio ambiant/musique conjointement avec la vid√©o (h√©rit√© de LTX-Video 2.0 audio branch), mais :
- Pas de synth√®se vocale
- Pas de dialogue
- Pas de voix-off

### Qualit√© comparable au march√© ?

üü† L'audio VAE de LTX-Video 2.0 est fonctionnel mais **pas au niveau** de ElevenLabs, Bark, ou XTTS pour le TTS. Il produit des ambiances sonores synchronis√©es avec la vid√©o, pas de la parole.

### Lip-sync interne ou post-processing ?

üî¥ **Inexistant.** Aucun module de lip-sync.

### Mixage automatis√© cr√©dible ?

üî¥ Le module `multimodal_coherence/` est structurel. Pas de mixage audio automatis√© fonctionnel.

### Gestion dynamique musique / ambiance ?

üü† La g√©n√©ration audio conjointe (via les cross-modal attention blocks `audio_to_video_attn`, `video_to_audio_attn`) produit de l'audio contextualis√©, mais sans contr√¥le granulaire musique/ambiance/dialogue.

---

## 7. Montage & √âtalonnage

### Timeline g√©n√©r√©e automatiquement ?

üî¥ **Non impl√©ment√©.** Le module `video_editing/` (~1 034 lignes) dans inference contient :
- Content analysis
- Dataset validation  
- Quality checking

Mais **aucun montage automatis√©** : pas de cuts, pas de transitions, pas de timeline multi-clips.

### Gestion LUT

üî¥ Inexistante.

### Color science ma√Ætris√©e

üî¥ Inexistante. Pas de color grading, pas d'√©talonnage, pas de color spaces (les vid√©os sont g√©n√©r√©es en sRGB sans post-processing couleur).

### Pipeline HDR

üî¥ Inexistant.

### Formats export

‚úÖ Export H.264 + AAC via PyAV/FFmpeg (`media_io.py`). Format unique, pas de ProRes, DNxHR, EXR, ou formats cin√©matographiques.

---

## 8. Scalabilit√© & Compute

### Co√ªt training initial estim√©

| Sc√©nario | Co√ªt |
|----------|------|
| Fine-tuning LoRA (capacit√© actuelle) | ~$20-100K (10-50h A100) |
| Entra√Ænement from scratch du transformer 1.9B | ~$1-4M (500-2000h A100) |
| Stack compl√®te from scratch (transformer + VAE + audio) | ~$1.3-5.4M |
| **Avec it√©rations R&D r√©alistes (3-5 tentatives)** | **$5-20M** |

### Co√ªt inference par vid√©o 30s

| GPU | Co√ªt estim√© |
|-----|-------------|
| A100 80GB (cloud) | ~$0.50-2.00 par vid√©o 30s (5-15 min @ $2/h) |
| H100 (cloud) | ~$0.30-1.50 par vid√©o 30s |
| Consumer GPU (RTX 4090) | Temps √ó co√ªt √©lectricit√© local |

### Risque d'explosion GPU

üü† **Mod√©r√©.** Le mod√®le complet (6-7B params total) n√©cessite :
- Minimum 24 GB VRAM pour inference FP8
- 40-80 GB VRAM sans quantization
- Le tiled VAE decoding et FP8 quantization r√©duisent significativement la charge m√©moire

### Besoin estim√© en A100 / H100

| Usage | Besoin |
|-------|--------|
| Fine-tuning LoRA | 1√ó A100/H100 (batch_size=1, gradient accumulation) |
| Inference SaaS (100 req/jour) | 2-4√ó A100 |
| Inference SaaS (1000 req/jour) | 10-20√ó A100 |
| Training from scratch | 32-128√ó A100 pendant 1-4 semaines |

### Optimisations possibles

- ‚úÖ FP8 quantization (d√©j√† impl√©ment√©)
- ‚úÖ Tiled VAE (d√©j√† impl√©ment√©)
- üü° Distillation du transformer (pipeline existe, pas de mod√®le distill√©)
- üü° Batching inference (non impl√©ment√©)
- üü° Speculative decoding (non impl√©ment√©)
- üü° TensorRT/ONNX export (non impl√©ment√©)

### Viabilit√© SaaS sans lev√©e massive

üî¥ **Non viable sans lev√©e significative.**

- Le Dockerfile Cloud Run **n'alloue aucun GPU** (4 CPU, 8 GB RAM) ‚Äî le SaaS actuel est un orchestrateur API, pas un service de rendu
- Si rendu via APIs externes (Veo-3, Runway) : co√ªts variables mais d√©pendance tier totale
- Si rendu local : infrastructure GPU d√©di√©e n√©cessaire (~$50-200K/an pour un cluster modeste)
- Budget .env.example : $10K/mois, $500/jour ‚Äî insuffisant pour un volume SaaS significatif avec mod√®les locaux

---

## 9. Robustesse r√©elle

### Comportement si node GPU crash

üî¥ **Aucun m√©canisme.** Pas de health check GPU au niveau service, pas de failover, pas de migration de job.

### Comportement si OOM

üü† Le tiled VAE decoding et FP8 r√©duisent le risque. Pas de gestion OOM gracieuse dans les pipelines (pas de try/catch autour de l'allocation VRAM, pas de fallback r√©solution inf√©rieure).

### Comportement si timeout

‚úÖ Le pipeline SaaS a des timeouts configurables (Cloud Run : 3600s max). üî¥ Pas de timeout c√¥t√© inference locale.

### Comportement si corruption dataset

üü† Les scripts de preprocessing valident les formats vid√©o mais pas de checksum ou de v√©rification d'int√©grit√© post-processing.

### Comportement si drift mod√®le

üî¥ **Aucun monitoring de drift.** Aucune m√©trique de qualit√© collect√©e en continu. Le module `reward_modeling/` (~513 lignes) est structurel.

### Comportement si d√©croissance qualit√©

üî¥ Pas de d√©tection automatique. Pas de A/B testing fonctionnel (module existe mais non connect√©).

### Capacit√© de monitoring

üü° GPU monitoring via `nvidia-smi` scripts. Pas de monitoring applicatif Prometheus/Grafana. WandB configur√© mais 0 run logged.

### Capacit√© de rollback

‚úÖ Checkpoint manager JSON pour l'orchestrateur SaaS. üî¥ Pas de rollback mod√®le (pas de model registry, pas de versioning des poids).

### Versioning mod√®le

üî¥ **Inexistant.** Pas de MLflow, pas de model registry, pas de versioning des checkpoints beyond le filesystem.

---

## 10. Failles critiques identifi√©es

### üî¥ Critique

1. **Le mod√®le fondamental n'est pas propri√©taire.** L'int√©gralit√© du moteur de production (`aiprod_core`) est un fork de LTX-Video 2.0 (Lightricks) avec remplacement de cha√Ænes. Les notices de copyright et licences ont √©t√© supprim√©es. Cela pose un risque juridique majeur et invalide l'argument commercial de "mod√®le propri√©taire".

2. **Aucune capacit√© d'entra√Ænement from scratch.** Le seul training fonctionnel est du fine-tuning LoRA sur un mod√®le pr√©-entra√Æn√© tiers. Le prototype `train.py` entra√Æne un toy model inutilisable. Aucun dataset propri√©taire n'existe.

3. **Le SaaS d√©ploy√© n'a pas de GPU.** Le Dockerfile Cloud Run d√©ploie un orchestrateur CPU-only qui d√©l√®gue le rendu √† des APIs tierces (Veo-3, Runway, Replicate). Ce n'est pas un syst√®me d'inf√©rence proprietaire.

4. **Gap massif entre le code r√©el et l'ambition.** ~62 000 lignes de code d'infrastructure dans `aiprod-pipelines/inference/` (SaaS multi-tenant, tensor parallelism, distributed LoRA, edge deployment, reward modeling) sont des **structures non connect√©es** : classes avec logique interne mais sans int√©gration au pipeline r√©el. Les inference nodes retournent `torch.randn()`.

5. **Risque juridique sur la propri√©t√© intellectuelle.** LTX-Video 2.0 est distribu√© sous licence Apache 2.0 qui requiert : pr√©servation des notices de copyright, attribution dans les fichiers NOTICE, indication des modifications. **Aucune de ces obligations n'est respect√©e.**

6. **Aucune capacit√© TTS, lip-sync, montage, √©talonnage, HDR.** Pour un produit qui "doit produire des vid√©os cin√©matographiques 100% finalis√©es automatiquement", ces composants sont enti√®rement absents.

### üü† Majeur

7. **Dualit√© architecturale non r√©solue.** Deux syst√®mes coexistent sans lien : un pipeline local GPU (fonctionnel mais non d√©ploy√©) et un SaaS Cloud Run (d√©ploy√© mais sans GPU). Pas de strat√©gie claire sur lequel constitue le produit.

8. **Aucune trace d'ex√©cution r√©elle.** R√©pertoire `logs/` vide, 0 run WandB, le seul checkpoint est un toy model de 152 MB. Aucune preuve que le syst√®me a jamais g√©n√©r√© une vid√©o avec le moteur de production.

9. **Tests unitaires incomplets pour le c≈ìur.** `aiprod-core/tests/` ne contient qu'un `conftest.py` ‚Äî les tests d√©crits dans le README n'existent pas. Les tests de `aiprod-pipelines` mockent `torch` enti√®rement, emp√™chant la validation GPU r√©elle.

10. **Inference nodes mock√©es.** Le syst√®me d'inference graph (`aiprod-pipelines/inference/nodes.py`) ‚Äî cens√© √™tre le c≈ìur de l'ex√©cution ‚Äî retourne `torch.randn()` pour toutes les op√©rations (encode, denoise, decode, upsample). Les presets construisent des graphes sur des nodes factices.

11. **Monitoring et observabilit√© absents.** Pas de Prometheus, Grafana, alerting. Pas de model drift detection. Pas de quality metrics collection en production.

12. **Pas de versioning mod√®le.** Aucun MLflow, model registry, ou m√©canisme de rollback des poids du mod√®le.

### üü° Mineur

13. **Monkey-patching dangereux.** `curriculum.py` injecte des fake modules dans `sys.modules` pour contourner `torch._dynamo`. Fragile et source potentielle de bugs silencieux.

14. **Config template dupliqu√©e.** `config/templates/pyproject.template.toml` est une copie exacte du `pyproject.toml` racine ‚Äî pas de templating r√©el.

15. **R√©pertoires scaffolding vides.** `scripts/data/`, `scripts/deployment/`, `scripts/dev/`, `scripts/maintenance/`, `scripts/testing/`, `deploy/kubernetes/`, `deploy/scripts/` ‚Äî pure structure sans contenu.

16. **Pas de batching inference.** Chaque requ√™te est trait√©e s√©quentiellement. Impact direct sur le throughput SaaS.

17. **Format d'export unique.** H.264 + AAC uniquement. Pas de ProRes, DNxHR, ou formats professionnels attendus pour un produit "cin√©matographique".

---

## 11. Recommandations prioritaires

### Top 5 corrections obligatoires avant tout lancement

1. **R√©soudre le statut juridique du fork LTX-Video 2.0.** Soit restaurer les attributions Apache 2.0 et communiquer honn√™tement que le mod√®le est bas√© sur LTX-Video, soit effectivement entra√Æner un mod√®le from scratch (co√ªt : $5-20M). Ne pas lever de fonds en pr√©sentant un fork renomm√© comme un mod√®le propri√©taire ‚Äî c'est un risque de due diligence fatal.

2. **Choisir une architecture de d√©ploiement unique.** Pipeline local GPU ou SaaS orchestrateur API ? Les deux approches sont l√©gitimes mais incompatibles dans leur √©tat actuel. Si SaaS avec GPU propre : budgetiser l'infrastructure. Si SaaS avec APIs tierces : assumer la d√©pendance et optimiser les co√ªts.

3. **Connecter les inference nodes.** Remplacer les `torch.randn()` dans `nodes.py` par les appels r√©els aux pipelines (`TI2VidTwoStagesPipeline`, etc.). Sans cela, le graph d'inf√©rence (qui repr√©sente ~60% de la codebase) est non-fonctionnel.

4. **Impl√©menter les composants manquants pour le "cin√©matographique".** TTS, lip-sync, montage automatis√©, √©talonnage, color grading, export multi-format sont des pr√©requis non n√©gociables pour la proposition de valeur annonc√©e.

5. **Ex√©cuter le pipeline de bout en bout et sauvegarder les preuves.** G√©n√©rer des vid√©os avec le moteur r√©el, logger les m√©triques, collecter des benchmarks de qualit√©/latence/co√ªt. Sans cela, toute d√©monstration ou lev√©e repose sur du fictif.

### Optimisations court terme (0-3 mois)

- Impl√©menter le batching inference
- Ajouter TensorRT/ONNX export pour r√©duction latence 2-5√ó
- Configurer Prometheus + Grafana pour monitoring GPU/inference
- √âcrire les tests unitaires du core (`aiprod-core/tests/`)
- Nettoyer les ~62 000 lignes de code infrastructure non connect√©

### Strat√©gie compute moyen terme (3-12 mois)

- Si mod√®le propri√©taire : constituer un dataset vid√©o/audio sous licence ($500K-2M), lancer l'entra√Ænement sur cluster A100/H100 ($2-5M)
- Si fork LTX-Video (honn√™te) : d√©velopper des LoRA sp√©cialis√©es de haute qualit√© comme diff√©renciateur, investir dans le pipeline de post-production
- Impl√©menter la distillation pour r√©duire le co√ªt inference de 3-5√ó
- D√©ployer sur une infrastructure GPU auto-scalable (GKE avec GPU nodes, ou RunPod/Lambda)

### Strat√©gie R&D long terme (12-36 mois)

- D√©velopper un mod√®le architectural r√©ellement propri√©taire si le positionnement l'exige
- Investir dans TTS/lip-sync propri√©taire pour la diff√©renciation
- Construire un pipeline de montage/√©talonnage automatis√© end-to-end
- D√©velopper des capacit√©s multi-sc√®nes avec coh√©rence narrative
- Impl√©menter l'A/B testing et le reward modeling pour l'am√©lioration continue

---

## 12. Score final

| Crit√®re | Score | Justification |
|---------|-------|---------------|
| **Solidit√© mod√®le** | 2/10 | Pas de mod√®le propri√©taire. Fork renomm√© sans attribution. Aucun training from scratch. Toy model non fonctionnel. |
| **Solidit√© infrastructure** | 3/10 | Pipeline local fonctionnel h√©rit√© de LTX-Video. SaaS orchestrateur sans GPU. ~62K lignes d'infrastructure non connect√©e. Monitoring absent. |
| **Viabilit√© √©conomique** | 2/10 | Pas de mod√®le propre = pas de moat technologique. D√©pendance totale sur mod√®les/APIs tiers. Co√ªts training from scratch : $5-20M. Co√ªts infra GPU SaaS : $50-200K/an minimum. |

### Probabilit√© que le SaaS survive 12 mois sans lev√©e massive

**< 5%** dans la configuration actuelle.

- Sans mod√®le propri√©taire, le produit est un wrapper sur LTX-Video 2.0 que n'importe qui peut reproduire
- Sans GPU dans le d√©ploiement, le SaaS d√©pend d'APIs tierces dont les co√ªts et la disponibilit√© ne sont pas contr√¥l√©s
- La dette technique (62K lignes non connect√©es, tests manquants, composants critiques absents) n√©cessite 6-12 mois de travail d'ing√©nierie avant un MVP cr√©dible
- Le risque juridique sur le fork non attribu√© est un showstopper pour toute due diligence

### Verdict

> üëâ **Irr√©aliste sans capitaux massifs ET r√©orientation strat√©gique fondamentale.**
>
> AIPROD dans son √©tat actuel est un **fork renomm√© de LTX-Video 2.0 envelopp√© dans une couche d'orchestration SaaS partiellement impl√©ment√©e**. Il ne poss√®de aucun mod√®le propri√©taire, n'a jamais entra√Æn√© de mod√®le from scratch, et n'a produit aucune preuve d'ex√©cution r√©elle du pipeline de bout en bout.
>
> Le projet pr√©sente une ambition de niveau Big Tech (mod√®les fondamentaux propri√©taires, pipeline cin√©matographique end-to-end, SaaS mondial) sans les ressources correspondantes (pas de dataset, pas de compute, pas d'√©quipe ML visible, pas de mod√®le entra√Æn√©).
>
> La voie la plus r√©aliste vers un produit viable serait d'**assumer honn√™tement la base LTX-Video 2.0**, se concentrer sur la **valeur ajout√©e de l'orchestration et du post-processing**, et investir dans les **composants manquants** (TTS, montage, √©talonnage) plut√¥t que de pr√©tendre √† un mod√®le propri√©taire inexistant.

---

*Fin de l'audit ‚Äî 14 f√©vrier 2026*
