"""
Pipeline Worker - Consomme les jobs depuis Pub/Sub et les ex√©cute.

P1.2.3: Background worker qui:
1. Consomme les messages JobMessage depuis aiprod-pipeline-jobs subscription
2. Ex√©cute le pipeline via state_machine.run()
3. Publie les r√©sultats vers aiprod-pipeline-results
4. Met √† jour le job status en PostgreSQL
5. G√®re les erreurs et les envoie vers DLQ
"""

import os
import sys
import json
import logging
import time
import asyncio
from typing import Optional, Callable, Any
from datetime import datetime, timedelta
from concurrent import futures
from functools import wraps

# Add src to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from google.cloud import pubsub_v1
from google.api_core.exceptions import GoogleAPICallError

from src.orchestrator.state_machine import StateMachine
from src.db.job_repository import JobRepository
from src.db.models import get_session_factory, JobState
from src.pubsub.client import get_pubsub_client, JobMessage, ResultMessage
from src.api.functions.input_sanitizer import InputSanitizer

logger = logging.getLogger(__name__)


class PipelineWorker:
    """Worker qui traite les jobs depuis Pub/Sub."""

    def __init__(self, project_id: Optional[str] = None, num_threads: int = 5):
        """
        Initialise le worker.

        Args:
            project_id: GCP project ID (defaut: env var GOOGLE_CLOUD_PROJECT)
            num_threads: Nombre de threads pour traiter les messages
        """
        self.project_id = project_id or os.getenv(
            "GOOGLE_CLOUD_PROJECT", "aiprod-484120"
        )
        self.num_threads = num_threads

        # Clients
        self.pubsub_client = get_pubsub_client()
        self.subscriber = pubsub_v1.SubscriberClient()

        # Database
        self.db_url = os.getenv(
            "DATABASE_URL", "postgresql://aiprod:password@localhost:5432/aiprod_v33"
        )
        session_factory, _ = get_session_factory(self.db_url)
        self.session_factory = session_factory

        # State machine et sanitizer
        self.state_machine = StateMachine()
        self.input_sanitizer = InputSanitizer()

        # Subscription path
        self.subscription_path = self.subscriber.subscription_path(
            self.project_id, "aiprod-pipeline-jobs-sub"
        )

        logger.info(
            f"üöÄ PipelineWorker initialized (project={self.project_id}, threads={num_threads})"
        )

    def process_message(self, message: pubsub_v1.subscriber.message.Message) -> bool:  # type: ignore[misc]
        """
        Traite un message de job depuis Pub/Sub.

        Args:
            message: Message Pub/Sub contenant un JobMessage

        Returns:
            bool: True si succ√®s, False si erreur
        """
        start_time = time.time()
        job_id = None

        try:
            # 1. D√©coder le message
            data = json.loads(message.data.decode("utf-8"))
            job_msg = JobMessage.from_dict(data)
            job_id = job_msg.job_id

            if not job_id:
                logger.error("‚ùå Job ID is None, cannot process message")
                message.nack()
                return False

            logger.info(
                f"üì® Processing job {job_id} for user {job_msg.user_id} "
                f"with preset {job_msg.preset}"
            )

            # 2. Mettre √† jour le job status ‚Üí PROCESSING
            db_session = self.session_factory()
            try:
                job_repo = JobRepository(db_session)
                job_repo.update_job_state(
                    job_id, JobState.PROCESSING, reason="Worker starting processing"
                )
                logger.info(f"‚úÖ Job {job_id} state ‚Üí PROCESSING")
            finally:
                db_session.close()

            # 3. Ex√©cuter le pipeline
            logger.info(f"‚öôÔ∏è  Running pipeline for job {job_id}...")

            # Pr√©parer les donn√©es d'entr√©e
            input_data = {
                "content": job_msg.content,
                "preset": job_msg.preset,
                "_user_id": job_msg.user_id,
                "_job_id": job_id,
            }

            # Ajouter les m√©tadonn√©es
            if job_msg.metadata:
                input_data.update(job_msg.metadata)

            # Sanitize inputs
            sanitized = self.input_sanitizer.sanitize(input_data)

            # Ex√©cuter le state machine (async pipeline)
            result = asyncio.run(self.state_machine.run(sanitized))

            execution_time_ms = int((time.time() - start_time) * 1000)
            logger.info(f"‚úÖ Pipeline completed for {job_id} in {execution_time_ms}ms")

            # 4. Mettre √† jour le job avec le r√©sultat
            db_session = self.session_factory()
            try:
                job_repo = JobRepository(db_session)

                # R√©cup√©rer le job
                job = job_repo.get_job(job_id)
                if not job:
                    logger.error(f"‚ùå Job {job_id} not found after processing")
                    return False

                # Ajouter le r√©sultat
                job_repo.set_job_result(
                    job_id=job_id,
                    status="success",
                    output=result,
                    error_message=None,
                    processing_time_ms=execution_time_ms,
                )

                # Mettre √† jour le statut
                job_repo.update_job_state(
                    job_id,
                    JobState.COMPLETED,
                    reason="Pipeline execution completed successfully",
                )

                logger.info(f"‚úÖ Job {job_id} status ‚Üí COMPLETED")
            finally:
                db_session.close()

            # 5. Publier le r√©sultat vers Pub/Sub
            # (Message cr√©√© directement dans publish_result)

            result_msg_id = self.pubsub_client.publish_result(
                job_id=job_id,
                status="success",
                output=result,
                error_message=None,
                processing_time_ms=execution_time_ms,
            )

            logger.info(f"üì§ Result published for {job_id} (msg_id={result_msg_id})")

            # 6. Acknowledger le message
            message.ack()
            logger.info(f"‚úÖ Message acknowledged for job {job_id}")

            return True

        except Exception as e:
            execution_time_ms = int((time.time() - start_time) * 1000)
            logger.error(f"‚ùå Error processing job {job_id}: {str(e)}", exc_info=True)

            if job_id:
                try:
                    # Mettre √† jour le job comme FAILED
                    db_session = self.session_factory()
                    try:
                        job_repo = JobRepository(db_session)
                        job_repo.update_job_state(
                            job_id,
                            JobState.FAILED,
                            reason=f"Pipeline execution error: {str(e)}",
                        )
                        job_repo.set_job_result(
                            job_id=job_id,
                            status="error",
                            output=None,
                            error_message=str(e),
                            processing_time_ms=execution_time_ms,
                        )
                    finally:
                        db_session.close()
                except Exception as db_error:
                    logger.error(f"‚ùå Failed to update job status: {str(db_error)}")

                try:
                    # Publier vers DLQ
                    dlq_msg_id = self.pubsub_client.publish_dlq_message(
                        job_id=job_id,
                        reason="Pipeline execution error",
                        error=str(e),
                        metadata={"preset": "unknown"},
                    )
                    logger.warning(
                        f"‚ö†Ô∏è  Job {job_id} published to DLQ (msg_id={dlq_msg_id})"
                    )
                except Exception as dlq_error:
                    logger.error(f"‚ùå Failed to publish to DLQ: {str(dlq_error)}")

            # Nacker le message (renvoyer √† la queue pour retry)
            message.nack()
            logger.warning(f"‚ö†Ô∏è  Message nacked for job {job_id} (will be retried)")

            return False

    def start(self):
        """D√©marre le worker - consomme les messages ind√©finiment."""
        logger.info(f"üöÄ Starting worker listening on {self.subscription_path}...")

        # Cr√©er un streaming pull future
        streaming_pull_future = self.subscriber.subscribe(
            self.subscription_path,
            callback=self.process_message,
            flow_control=pubsub_v1.types.FlowControl(
                max_messages=self.num_threads, max_bytes=10 * 1024 * 1024  # 10MB
            ),
        )

        logger.info("‚úÖ Worker ready. Processing messages...")

        try:
            # Attendre ind√©finiment
            streaming_pull_future.result()
        except KeyboardInterrupt:
            logger.info("\nüõë Shutting down worker...")
            streaming_pull_future.cancel()
            streaming_pull_future.result()
            logger.info("‚úÖ Worker shutdown complete")
        except Exception as e:
            logger.error(f"‚ùå Worker error: {str(e)}")
            streaming_pull_future.cancel()
            streaming_pull_future.result()


def main():
    """Point d'entr√©e principal du worker."""
    import argparse

    parser = argparse.ArgumentParser(
        description="AIPROD V33 Pipeline Worker - Background job processor"
    )
    parser.add_argument(
        "--project",
        type=str,
        default=None,
        help="GCP Project ID (default: env var GOOGLE_CLOUD_PROJECT)",
    )
    parser.add_argument(
        "--threads", type=int, default=5, help="Number of worker threads (default: 5)"
    )

    args = parser.parse_args()

    # Initialiser le worker
    worker = PipelineWorker(project_id=args.project, num_threads=args.threads)

    # D√©marrer
    worker.start()


if __name__ == "__main__":
    main()
